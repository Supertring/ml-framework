{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disciplinary-retrieval",
   "metadata": {},
   "source": [
    "# Machine Learning Lab - Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-remark",
   "metadata": {},
   "source": [
    "**Summer Term 2021**\n",
    "\n",
    "- Julian Stier <julian.stier@uni-passau.de>\n",
    "- Sahib Julka <sahib.julka@uni-passau.de>\n",
    "- [StudIP Machine Learning Lab](https://studip.uni-passau.de/studip/dispatch.php/course/scm?cid=42befdd6822ee2029b26fa475cd02f60)\n",
    "- [FimGIT repositories](https://fimgit.fim.uni-passau.de/groups/padas/21ss-mllab/)\n",
    "\n",
    "**General Remarks**\n",
    "- You have time from 09:00 AM until 03:00 PM to work on the hackathon task.\n",
    "- Go through the notebook, answer questions, solve described tasks and fill out empty spaces or add cells based on your creativity.\n",
    "- Re-use previous implementations (of your own!) by either importing according python modules or copying it into the notebook.\n",
    "- Your overall git repository acts as the official submission. Put the hackathon notebook also into the git repository, alongside with any previous notebooks or python implementations you already uploaded.\n",
    "- If one of your implementation required for this notebook has not been working previously, you can now work on that specifically and try to solve it within the given time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "analyzed-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-merchandise",
   "metadata": {},
   "source": [
    "# Step I: Prepare Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-knife",
   "metadata": {},
   "source": [
    "- Download the two datasets.\n",
    "- Read it into memory.\n",
    "- Understand the feature shape and number of targets.\n",
    "- Split both datasets into three fixed train-validation-test sets with own chosen proportions. You can e.g. use 80% of the data for training, 10% of the data for the validation set and 10% for the test set. Make sure you shuffle the data in before once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-sherman",
   "metadata": {},
   "source": [
    "### UCI Dataset: Abalone\n",
    "> https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "canadian-underwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget -P ./data/abalone/ https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\n",
    "!wget -P ./data/abalone/ https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decent-third",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!cat datasets/abalone/abalone.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "express-stanford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\", \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
    "df = pd.read_csv(\"datasets/abalone/abalone.data\", header=None, names=col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf89e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed5e450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'F', 'I'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8090a3",
   "metadata": {},
   "source": [
    "__Convert target value to numerical form__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f86e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sex\"] = df[\"Sex\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2abb627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = df['Sex'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0947b603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
       "0    2   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1    2   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2    0   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3    2   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4    1   0.330     0.255   0.080        0.2050          0.0895   \n",
       "\n",
       "   Viscera weight  Shell weight  Rings  \n",
       "0          0.1010         0.150     15  \n",
       "1          0.0485         0.070      7  \n",
       "2          0.1415         0.210      9  \n",
       "3          0.1140         0.155     10  \n",
       "4          0.0395         0.055      7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3552d4",
   "metadata": {},
   "source": [
    "__shuffle datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6adebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c6549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.0840</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.030</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2665</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.075</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.7930</td>\n",
       "      <td>0.4340</td>\n",
       "      <td>0.1405</td>\n",
       "      <td>0.190</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0375</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>0.283</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.2115</td>\n",
       "      <td>0.239</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
       "0    2   0.265     0.200   0.065        0.0840          0.0340   \n",
       "1    2   0.380     0.285   0.100        0.2665          0.1150   \n",
       "2    2   0.545     0.410   0.120        0.7930          0.4340   \n",
       "3    2   0.615     0.475   0.150        1.0375          0.4760   \n",
       "4    1   0.575     0.450   0.135        0.8245          0.3375   \n",
       "\n",
       "   Viscera weight  Shell weight  Rings  \n",
       "0          0.0105         0.030      7  \n",
       "1          0.0610         0.075     11  \n",
       "2          0.1405         0.190      9  \n",
       "3          0.2325         0.283      9  \n",
       "4          0.2115         0.239     11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f32765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0], dtype=int8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10819a",
   "metadata": {},
   "source": [
    "__split data to train validation and test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e473e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:3340]\n",
    "valid = df[3340:3760]\n",
    "test = df[3760:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b9880",
   "metadata": {},
   "source": [
    "__Convert into numpy array__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21a0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "uci_features_train = np.array(train.drop(['Sex'],1))\n",
    "train_label = train[['Sex']]\n",
    "uci_labels_train = np.array(np.array(train_label, dtype=None).reshape(-1))\n",
    "\n",
    "uci_features_valid = np.array(valid.drop(['Sex'],1))\n",
    "valid_label = valid[['Sex']]\n",
    "uci_labels_valid = np.array(np.array(valid_label, dtype=None).reshape(-1))\n",
    "\n",
    "uci_features_test = np.array(test.drop(['Sex'],1))\n",
    "test_label = test[['Sex']]\n",
    "uci_labels_test = np.array(np.array(test_label, dtype=None).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f48277",
   "metadata": {},
   "source": [
    "__data in numpy array__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a079e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.650e-01, 2.000e-01, 6.500e-02, ..., 1.050e-02, 3.000e-02,\n",
       "        7.000e+00],\n",
       "       [3.800e-01, 2.850e-01, 1.000e-01, ..., 6.100e-02, 7.500e-02,\n",
       "        1.100e+01],\n",
       "       [5.450e-01, 4.100e-01, 1.200e-01, ..., 1.405e-01, 1.900e-01,\n",
       "        9.000e+00],\n",
       "       ...,\n",
       "       [3.600e-01, 3.000e-01, 8.500e-02, ..., 6.400e-02, 7.450e-02,\n",
       "        7.000e+00],\n",
       "       [4.500e-01, 3.500e-01, 1.450e-01, ..., 1.000e-01, 1.655e-01,\n",
       "        1.500e+01],\n",
       "       [5.600e-01, 4.450e-01, 1.550e-01, ..., 2.090e-01, 2.750e-01,\n",
       "        1.600e+01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uci_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea29ea24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.265, 0.2  , 0.065],\n",
       "       [0.38 , 0.285, 0.1  ],\n",
       "       [0.545, 0.41 , 0.12 ],\n",
       "       ...,\n",
       "       [0.36 , 0.3  , 0.085],\n",
       "       [0.45 , 0.35 , 0.145],\n",
       "       [0.56 , 0.445, 0.155]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uci_features_train[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c527f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 1, 1], dtype=int8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uci_labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-express",
   "metadata": {},
   "source": [
    "### Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "424fc2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget -P ./data/fashion/raw/ https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-images-idx3-ubyte.gz\n",
    "!wget -P ./data/fashion/raw/ https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-labels-idx1-ubyte.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-willow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "complex-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_features_train = np.array([])\n",
    "fmnist_labels_train = np.array([])\n",
    "\n",
    "fmnist_features_valid = np.array([])\n",
    "fmnist_labels_valid = np.array([])\n",
    "\n",
    "fmnist_features_test = np.array([])\n",
    "fmnist_labels_test = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-accent",
   "metadata": {},
   "source": [
    "# Step II: Choose a Baseline Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-cemetery",
   "metadata": {},
   "source": [
    "* Choose a baseline classifier - except the neural network classifier - you have been working with over the semester and let it learn based on the **small** dataset\n",
    "* Provide some error measure or indicator whether your classifier learned, e.g. loss over multiple steps or the number of correctly classified samples on the training set or similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-multimedia",
   "metadata": {},
   "source": [
    "```python\n",
    "model_baseline = YourAlgorithm()\n",
    "model_baseline.learn(uci_features_train, uci_labels_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d79f6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllab.SimpleNN import SimpleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8756a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 3.473898\n",
      "Loss after iteration 0: 1.384530\n",
      "Loss after iteration 0: 3.037768\n",
      "Loss after iteration 0: 2.230842\n",
      "Loss after iteration 0: 1.318337\n",
      "Loss after iteration 0: 1.218905\n",
      "Loss after iteration 0: 1.779133\n",
      "Loss after iteration 0: 3.053287\n",
      "Loss after iteration 0: 1.530960\n",
      "Loss after iteration 0: 1.147774\n",
      "Loss after iteration 0: 1.046585\n",
      "Loss after iteration 0: 1.146571\n",
      "Loss after iteration 0: 1.214444\n",
      "Loss after iteration 0: 1.176012\n",
      "Loss after iteration 0: 1.161160\n",
      "Loss after iteration 0: 1.184322\n",
      "Loss after iteration 0: 1.149983\n",
      "Loss after iteration 0: 1.159964\n",
      "Loss after iteration 0: 1.113246\n",
      "Loss after iteration 0: 1.105006\n",
      "Loss after iteration 0: 1.162852\n",
      "Loss after iteration 0: 1.181030\n",
      "Loss after iteration 0: 1.156134\n",
      "Loss after iteration 0: 1.188966\n",
      "Loss after iteration 0: 1.237562\n",
      "Loss after iteration 0: 1.172098\n",
      "Loss after iteration 0: 1.235979\n",
      "Loss after iteration 0: 1.109781\n",
      "Loss after iteration 0: 1.102034\n",
      "Loss after iteration 0: 1.089430\n",
      "Loss after iteration 0: 1.153360\n",
      "Loss after iteration 0: 1.023949\n",
      "Loss after iteration 0: 1.304612\n",
      "Loss after iteration 0: 1.166089\n",
      "Loss after iteration 0: 1.154614\n",
      "Loss after iteration 0: 1.150369\n",
      "Loss after iteration 0: 1.160756\n",
      "Loss after iteration 0: 1.158482\n",
      "Loss after iteration 0: 1.175064\n",
      "Loss after iteration 0: 1.327448\n",
      "Loss after iteration 0: 1.235484\n",
      "Loss after iteration 0: 1.143164\n",
      "Loss after iteration 0: 1.139286\n",
      "Loss after iteration 0: 1.127951\n",
      "Loss after iteration 0: 1.076368\n",
      "Loss after iteration 0: 1.159588\n",
      "Loss after iteration 0: 1.150879\n",
      "Loss after iteration 0: 1.114931\n",
      "Loss after iteration 0: 1.160642\n",
      "Loss after iteration 0: 1.168709\n",
      "Loss after iteration 0: 1.029038\n",
      "Loss after iteration 0: 1.147316\n",
      "Loss after iteration 0: 1.172727\n",
      "Loss after iteration 0: 1.131471\n",
      "Loss after iteration 0: 1.209236\n",
      "Loss after iteration 0: 1.289083\n",
      "Loss after iteration 0: 1.213911\n",
      "Loss after iteration 0: 1.152253\n",
      "Loss after iteration 0: 1.082934\n",
      "Loss after iteration 0: 1.162378\n",
      "Loss after iteration 0: 1.165825\n",
      "Loss after iteration 0: 1.139060\n",
      "Loss after iteration 0: 1.130462\n",
      "Loss after iteration 0: 1.084340\n",
      "Loss after iteration 0: 1.209657\n",
      "Loss after iteration 0: 1.151494\n",
      "Loss after iteration 0: 1.067700\n",
      "Loss after iteration 0: 1.143975\n",
      "Loss after iteration 0: 1.191727\n",
      "Loss after iteration 0: 1.110590\n",
      "Loss after iteration 0: 1.120243\n",
      "Loss after iteration 0: 1.126964\n",
      "Loss after iteration 0: 1.103462\n",
      "Loss after iteration 0: 1.137833\n",
      "Loss after iteration 0: 1.163713\n",
      "Loss after iteration 0: 1.141160\n",
      "Loss after iteration 0: 1.126769\n",
      "Loss after iteration 0: 1.129717\n",
      "Loss after iteration 0: 1.101208\n",
      "Loss after iteration 0: 1.139613\n",
      "Loss after iteration 0: 1.092894\n",
      "Loss after iteration 0: 1.160536\n",
      "Loss after iteration 0: 1.123831\n",
      "Loss after iteration 0: 1.136122\n",
      "Loss after iteration 0: 1.123496\n",
      "Loss after iteration 0: 0.933083\n",
      "Loss after iteration 0: 1.146079\n",
      "Loss after iteration 0: 1.011929\n",
      "Loss after iteration 0: 1.041208\n",
      "Loss after iteration 0: 1.042635\n",
      "Loss after iteration 0: 1.081240\n",
      "Loss after iteration 0: 1.139435\n",
      "Loss after iteration 0: 1.099144\n",
      "Loss after iteration 0: 1.147866\n",
      "Loss after iteration 0: 1.175504\n",
      "Loss after iteration 0: 1.132144\n",
      "Loss after iteration 0: 1.123718\n",
      "Loss after iteration 0: 1.121801\n",
      "Loss after iteration 0: 1.100424\n",
      "Loss after iteration 0: 1.137179\n",
      "Loss after iteration 0: 1.114630\n",
      "Loss after iteration 0: 1.132119\n",
      "Loss after iteration 0: 1.105401\n",
      "Loss after iteration 0: 1.132259\n",
      "Loss after iteration 0: 1.093180\n"
     ]
    }
   ],
   "source": [
    "nn = SimpleNN()\n",
    "nn.add(activation=\"tanh\",input_dim=8, hidden_dim=16, output_dim=3, print_loss=True)\n",
    "nn.train(uci_features_train, uci_labels_train, epoches=2000, reg_value=0.01, epsilon=0.01, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e8c5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = nn.infer(uci_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b3b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.00 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy: %2.2f %%' % (100. * metrics.accuracy_score(uci_labels_test, pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a845734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllab.Hackathon import SimpleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SimpleNN()\n",
    "nn.add(activation=\"tanh\",input_dim=8, hidden_dim=16, output_dim=3, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3dd814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.548827\n",
      "Loss after iteration 0: 1.152364\n",
      "Loss after iteration 0: 1.719223\n",
      "Loss after iteration 0: 1.150926\n",
      "Loss after iteration 0: 1.374045\n",
      "Loss after iteration 0: 1.267933\n",
      "Loss after iteration 0: 1.094693\n",
      "Loss after iteration 0: 1.093733\n",
      "Loss after iteration 0: 1.079807\n",
      "Loss after iteration 0: 1.221232\n",
      "Loss after iteration 0: 1.196634\n",
      "Loss after iteration 0: 1.180083\n",
      "Loss after iteration 0: 1.082463\n",
      "Loss after iteration 0: 1.128235\n",
      "Loss after iteration 0: 1.114035\n",
      "Loss after iteration 0: 1.126828\n",
      "Loss after iteration 0: 1.134093\n",
      "Loss after iteration 0: 1.109282\n",
      "Loss after iteration 0: 1.095049\n",
      "Loss after iteration 0: 1.110274\n",
      "Loss after iteration 0: 1.236029\n",
      "Loss after iteration 0: 1.146087\n",
      "Loss after iteration 0: 1.207606\n",
      "Loss after iteration 0: 1.052651\n",
      "Loss after iteration 0: 1.180192\n",
      "Loss after iteration 0: 1.189899\n",
      "Loss after iteration 0: 1.055681\n",
      "Loss after iteration 0: 1.095291\n",
      "Loss after iteration 0: 1.095548\n",
      "Loss after iteration 0: 1.086695\n",
      "Loss after iteration 0: 1.088560\n",
      "Loss after iteration 0: 1.106424\n",
      "Loss after iteration 0: 1.175393\n",
      "Loss after iteration 0: 1.243373\n",
      "Loss after iteration 0: 1.097151\n",
      "Loss after iteration 0: 1.113180\n",
      "Loss after iteration 0: 1.096242\n",
      "Loss after iteration 0: 1.059166\n",
      "Loss after iteration 0: 1.142574\n",
      "Loss after iteration 0: 1.112661\n",
      "Loss after iteration 0: 1.059212\n",
      "Loss after iteration 0: 1.090639\n",
      "Loss after iteration 0: 1.065742\n",
      "Loss after iteration 0: 1.032321\n",
      "Loss after iteration 0: 1.057512\n",
      "Loss after iteration 0: 1.123234\n",
      "Loss after iteration 0: 1.137403\n",
      "Loss after iteration 0: 1.184120\n",
      "Loss after iteration 0: 1.181978\n",
      "Loss after iteration 0: 1.102441\n",
      "Loss after iteration 0: 1.114197\n",
      "Loss after iteration 0: 1.114884\n",
      "Loss after iteration 0: 1.108694\n",
      "Loss after iteration 0: 1.231295\n",
      "Loss after iteration 0: 1.053439\n",
      "Loss after iteration 0: 1.235498\n",
      "Loss after iteration 0: 1.169480\n",
      "Loss after iteration 0: 1.078344\n",
      "Loss after iteration 0: 1.097691\n",
      "Loss after iteration 0: 1.134898\n",
      "Loss after iteration 0: 1.087189\n",
      "Loss after iteration 0: 0.991772\n",
      "Loss after iteration 0: 1.146673\n",
      "Loss after iteration 0: 1.113003\n",
      "Loss after iteration 0: 1.089561\n",
      "Loss after iteration 0: 1.049761\n",
      "Loss after iteration 0: 1.046207\n",
      "Loss after iteration 0: 1.122170\n",
      "Loss after iteration 0: 1.106423\n",
      "Loss after iteration 0: 1.076566\n",
      "Loss after iteration 0: 1.073992\n",
      "Loss after iteration 0: 1.141397\n",
      "Loss after iteration 0: 1.136776\n",
      "Loss after iteration 0: 1.087077\n",
      "Loss after iteration 0: 1.096321\n",
      "Loss after iteration 0: 1.102456\n",
      "Loss after iteration 0: 1.078506\n",
      "Loss after iteration 0: 1.185797\n",
      "Loss after iteration 0: 1.046657\n",
      "Loss after iteration 0: 1.069684\n",
      "Loss after iteration 0: 0.956505\n",
      "Loss after iteration 0: 1.172975\n",
      "Loss after iteration 0: 1.129288\n",
      "Loss after iteration 0: 1.094158\n",
      "Loss after iteration 0: 1.105775\n",
      "Loss after iteration 0: 1.172623\n",
      "Loss after iteration 0: 1.105107\n",
      "Loss after iteration 0: 1.191605\n",
      "Loss after iteration 0: 1.087767\n",
      "Loss after iteration 0: 1.183915\n",
      "Loss after iteration 0: 1.088182\n",
      "Loss after iteration 0: 1.132171\n",
      "Loss after iteration 0: 1.030043\n",
      "Loss after iteration 0: 1.225409\n",
      "Loss after iteration 0: 1.112329\n",
      "Loss after iteration 0: 1.205534\n",
      "Loss after iteration 0: 1.160502\n",
      "Loss after iteration 0: 1.133703\n",
      "Loss after iteration 0: 1.097378\n",
      "Loss after iteration 0: 1.113195\n",
      "Loss after iteration 0: 1.118257\n",
      "Loss after iteration 0: 1.135392\n",
      "Loss after iteration 0: 1.100933\n",
      "Loss after iteration 0: 1.074694\n",
      "Loss after iteration 0: 1.094027\n"
     ]
    }
   ],
   "source": [
    "nn.train(uci_features_train, uci_labels_train, epoches=2000, reg_value=0.01, epsilon=0.01, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f4ec26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = nn.infer(uci_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f0df363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.36 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy: %2.2f %%' % (100. * metrics.accuracy_score(uci_labels_test, pred_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-synthesis",
   "metadata": {},
   "source": [
    "# Step III: Provide Evaluation Metrics for the Classifier Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-marketing",
   "metadata": {},
   "source": [
    "* Given the class interface for machine learning models, use the predicted target from the result of an model.infer()-invocation to calculate precision, recall and f1-score given the actual test-set targets.\n",
    "* Do not use scikit-learn or similar libraries; but you can orientate on such interfaces or implementations.\n",
    "* Note, that a model can return two or multiple classes based on the problem it learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-drinking",
   "metadata": {},
   "source": [
    "```python\n",
    "baseline_predicted = model_baseline.infer(features_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-budget",
   "metadata": {},
   "source": [
    "> https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "$precision = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}$\n",
    "\n",
    "$recall = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a56dc",
   "metadata": {},
   "source": [
    "__Precision, Recall & F1 score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ed2398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(testy, predy):\n",
    "    cls = np.unique(testy)\n",
    "    mat = np.zeros((len(testy), len(predy)))\n",
    "    for i in range(len(cls)):\n",
    "        for j in range(len(cls)):\n",
    "            print(testy)\n",
    "            print(cls[i])\n",
    "            mat[i, j] = np.sum((testy == cls[i]) & (predy == cls[j]))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b84de884",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(testy, predy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "37d5e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = np.array(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b07bdc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   7., 135., ...,   0.,   0.,   0.],\n",
       "       [  0.,  79.,  41., ...,   0.,   0.,   0.],\n",
       "       [  0.,  24., 131., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853df00",
   "metadata": {},
   "source": [
    "__True Positive__\n",
    "* Sum of all diagonal elements gives true positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "489256cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = np.trace(confusion)\n",
    "TP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3365b3",
   "metadata": {},
   "source": [
    "__False Positive__\n",
    "* sum all the columns values\n",
    "* subtract with the diagonal values\n",
    "* Remains False positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a768360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sum = np.sum(confusion, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7b9c1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = np.diag(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ef88cc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FP = col_sum - diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "813cf1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP = np.sum(FP)\n",
    "FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2701c",
   "metadata": {},
   "source": [
    "__False Negative__\n",
    "* sum all the rows values\n",
    "* subtract with the diagonal values\n",
    "* Remains False negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8170774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_sum = np.sum(confusion, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b16a2275",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diag = np.diag(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d6abed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FN = row_sum - diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "520f9918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FN = np.sum(FN)\n",
    "FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90831a",
   "metadata": {},
   "source": [
    "__True Negative__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5532fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall True negative is zero\n",
    "TN = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfbbbe",
   "metadata": {},
   "source": [
    "__Calculate TP, FP, FN, TN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4c4b9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(confusion):\n",
    "    #True positive\n",
    "    TP = np.trace(confusion)\n",
    "    \n",
    "    #False positive\n",
    "    col_sum = np.sum(confusion, axis=0)\n",
    "    diag = np.diag(confusion)\n",
    "    FP = col_sum - diag\n",
    "    FP = np.sum(FP)\n",
    "    \n",
    "    #false negative\n",
    "    row_sum = np.sum(confusion, axis=1)\n",
    "    diag = np.diag(confusion)\n",
    "    FN = row_sum - diag\n",
    "    FN = np.sum(FN)\n",
    "    \n",
    "    #True negative\n",
    "    TN = 0\n",
    "    \n",
    "    return TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ac899f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, FN, TN = TF(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ab843c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210.0, 207.0, 207.0, 0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cd473",
   "metadata": {},
   "source": [
    "__Precision__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4dfabbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(confusion):\n",
    "    TP, FP, FN, TN = TF(confusion)\n",
    "    precision = TP / (TP + FP)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cbeaae95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035971223021583"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Precision(confusion)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea053f7",
   "metadata": {},
   "source": [
    "__Recall__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0313ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(confusion):\n",
    "    TP, FP, FN, TN = TF(confusion)\n",
    "    recall = TP / (TP + FN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dcda4d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035971223021583"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Recall(confusion)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ee49e",
   "metadata": {},
   "source": [
    "__F1 - score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3235ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(confusion):\n",
    "    precision = Precision(confusion)\n",
    "    recall = Recall(confusion)\n",
    "    f1 = 2 * ((precision * recall)/(precision + recall))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6c67d27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035971223021583"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = F1_score(confusion)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "593f5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = np.array(testy).reshape(-1)\n",
    "predy = np.array(predy).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28b9d0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'testy' : testy,\n",
    "        'predy' : predy}\n",
    "\n",
    "df = pd.DataFrame(data, columns=['testy', 'predy'])\n",
    "confusion_matrix = pd.crosstab(df['testy'], df['predy'], rownames=['actual'], colnames=['predicted'], margins=True)\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-vacuum",
   "metadata": {},
   "source": [
    "### Testing your implementation\n",
    "You can use below vectors as a reference for testing the output of infer() and the target vector of a 10-class-classifier. The *f1_score* method of scikit learn gives you a reference on how the values need to look like. Using the function is of course not a valid solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_baseline_predicted = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 1, 2, 2, 3, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 5, 6, 7, 8, 9])\n",
    "example_baseline_test_target = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 3, 3, 3, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 4, 4, 5, 8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "vital-storage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.68695652, 0.56709957])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(testy, predy, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-municipality",
   "metadata": {},
   "source": [
    "# Step IV: Experiment (1) Hyperparameter Choice of Baseline Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-ordering",
   "metadata": {},
   "source": [
    "* Use one fixed train-validation-test split.\n",
    "* Choose a hyperparameter of your baseline classifier.\n",
    "* Conduct a grid search to find the best suitable value for it. Let the classifier learn on the training set and use an evaluation metric on the validation set (not the test set!) to find out which hyperparameter value works best for your classifier on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-manner",
   "metadata": {},
   "source": [
    "```python\n",
    "possible_hp_values = np.arange(1, 10, 0.1)\n",
    "best_hp_value = None\n",
    "best_f1_score = -np.infty\n",
    "for hp_value in possible_hp_values:\n",
    "    # 1. create a baseline classifier object with hp_value specified\n",
    "    current_model = YourAlgorithm(hyperparam=hp_value)\n",
    "    \n",
    "    # 2. learn the classifier on the training set\n",
    "    current_model.learn(uci_features_train, uci_labels_train)\n",
    "    \n",
    "    # 3. evaluate the model on the validation set\n",
    "    prediction = current_model.infer(uci_features_valid)\n",
    "    \n",
    "    f1_score = compute_f1(uci_labels_valid, prediction)\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        best_hp_value = hp_value\n",
    "\n",
    "print(\"Found hyperparameter value\", best_hp_value)\n",
    "print(\"Best f1-score on validation set\", best_f1_score)\n",
    "\n",
    "test_model = YourAlgorithm(hyperparam=best_hp_value)\n",
    "test_model.learn(uci_features_train, uci_labels_train)\n",
    "prediction = test_model.infer(uci_features_test)\n",
    "test_f1_score = compute_f1(uci_labels_test, prediction)\n",
    "print(\"F1-Score on test set\", test_f1_score)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c2aa5",
   "metadata": {},
   "source": [
    "__Batch size is changed__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c2be7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 3.637815\n",
      "Loss after iteration 0: 1.910060\n",
      "Loss after iteration 0: 4.183887\n",
      "Loss after iteration 0: 3.309771\n",
      "Loss after iteration 0: 3.656066\n",
      "Loss after iteration 0: 2.397539\n",
      "Loss after iteration 0: 2.634979\n",
      "Loss after iteration 0: 2.065391\n",
      "Loss after iteration 0: 1.488796\n",
      "Loss after iteration 0: 0.912948\n",
      "Loss after iteration 0: 1.038738\n",
      "Loss after iteration 0: 0.770882\n",
      "Loss after iteration 0: 1.020170\n",
      "Loss after iteration 0: 1.020745\n",
      "Loss after iteration 0: 1.097246\n",
      "Loss after iteration 0: 1.013267\n",
      "Loss after iteration 0: 1.092186\n",
      "Loss after iteration 0: 0.635765\n",
      "Loss after iteration 0: 1.031634\n",
      "Loss after iteration 0: 1.041832\n",
      "Loss after iteration 0: 1.080751\n",
      "Loss after iteration 0: 1.079451\n",
      "Loss after iteration 0: 1.079147\n",
      "Loss after iteration 0: 0.998578\n",
      "Loss after iteration 0: 0.775309\n",
      "Loss after iteration 0: 0.959161\n",
      "Loss after iteration 0: 1.088039\n",
      "Loss after iteration 0: 1.055747\n",
      "Loss after iteration 0: 0.975239\n",
      "Loss after iteration 0: 1.105002\n",
      "Loss after iteration 0: 1.037454\n",
      "Loss after iteration 0: 0.938885\n",
      "Loss after iteration 0: 0.848282\n",
      "Loss after iteration 0: 1.113089\n",
      "Loss after iteration 0: 0.971720\n",
      "Loss after iteration 0: 1.128043\n",
      "Loss after iteration 0: 1.016466\n",
      "Loss after iteration 0: 1.056370\n",
      "Loss after iteration 0: 0.905724\n",
      "Loss after iteration 0: 1.131764\n",
      "Loss after iteration 0: 1.000491\n",
      "Loss after iteration 0: 1.063340\n",
      "Loss after iteration 0: 1.042275\n",
      "Loss after iteration 0: 1.092281\n",
      "Loss after iteration 0: 0.991752\n",
      "Loss after iteration 0: 0.886505\n",
      "Loss after iteration 0: 0.988917\n",
      "Loss after iteration 0: 1.091930\n",
      "Loss after iteration 0: 1.038784\n",
      "Loss after iteration 0: 1.012391\n",
      "Loss after iteration 0: 0.976639\n",
      "Loss after iteration 0: 0.886764\n",
      "Loss after iteration 0: 0.974346\n",
      "Loss after iteration 0: 1.107530\n",
      "Loss after iteration 0: 1.087314\n",
      "Loss after iteration 0: 1.096992\n",
      "Loss after iteration 0: 1.084334\n",
      "Loss after iteration 0: 0.921470\n",
      "Loss after iteration 0: 1.089426\n",
      "Loss after iteration 0: 1.009150\n",
      "Loss after iteration 0: 1.059966\n",
      "Loss after iteration 0: 1.009613\n",
      "Loss after iteration 0: 1.010876\n",
      "Loss after iteration 0: 1.002660\n",
      "Loss after iteration 0: 1.056959\n",
      "Loss after iteration 0: 1.089226\n",
      "Loss after iteration 0: 1.085551\n",
      "Loss after iteration 0: 1.096272\n",
      "Loss after iteration 0: 1.084060\n",
      "Loss after iteration 0: 1.056580\n",
      "Loss after iteration 0: 1.084397\n",
      "Loss after iteration 0: 0.889340\n",
      "Loss after iteration 0: 1.083665\n",
      "Loss after iteration 0: 0.939350\n",
      "Loss after iteration 0: 1.081646\n",
      "Loss after iteration 0: 1.049706\n",
      "Loss after iteration 0: 1.090814\n",
      "Loss after iteration 0: 1.082790\n",
      "Loss after iteration 0: 0.809695\n",
      "Loss after iteration 0: 1.092307\n",
      "Loss after iteration 0: 1.121658\n",
      "Loss after iteration 0: 1.024109\n",
      "Loss after iteration 0: 1.013875\n",
      "Loss after iteration 0: 0.958526\n",
      "Loss after iteration 0: 1.085741\n",
      "Loss after iteration 0: 0.670507\n",
      "Loss after iteration 0: 0.956496\n",
      "Loss after iteration 0: 1.168676\n",
      "Loss after iteration 0: 1.001360\n",
      "Loss after iteration 0: 0.982883\n",
      "Loss after iteration 0: 1.083069\n",
      "Loss after iteration 0: 1.028335\n",
      "Loss after iteration 0: 1.033841\n",
      "Loss after iteration 0: 1.008733\n",
      "Loss after iteration 0: 0.850494\n",
      "Loss after iteration 0: 0.962766\n",
      "Loss after iteration 0: 1.157098\n",
      "Loss after iteration 0: 1.041960\n",
      "Loss after iteration 0: 1.005261\n",
      "Loss after iteration 0: 0.975313\n",
      "Loss after iteration 0: 1.112799\n",
      "Loss after iteration 0: 1.050585\n",
      "Loss after iteration 0: 0.875970\n",
      "Loss after iteration 0: 1.119337\n",
      "Loss after iteration 0: 0.989786\n",
      "Loss after iteration 0: 1.028001\n",
      "Loss after iteration 0: 0.986099\n",
      "Loss after iteration 0: 1.092750\n",
      "Loss after iteration 0: 1.121502\n",
      "Loss after iteration 0: 1.047963\n",
      "Loss after iteration 0: 1.025376\n",
      "Loss after iteration 0: 0.819121\n",
      "Loss after iteration 0: 1.115867\n",
      "Loss after iteration 0: 1.091381\n",
      "Loss after iteration 0: 1.092416\n",
      "Loss after iteration 0: 1.086361\n",
      "Loss after iteration 0: 1.042376\n",
      "Loss after iteration 0: 0.990090\n",
      "Loss after iteration 0: 1.080988\n",
      "Loss after iteration 0: 1.049343\n",
      "Loss after iteration 0: 1.084397\n",
      "Loss after iteration 0: 1.088501\n",
      "Loss after iteration 0: 0.994984\n",
      "Loss after iteration 0: 1.116230\n",
      "Loss after iteration 0: 1.080865\n",
      "Loss after iteration 0: 0.989525\n",
      "Loss after iteration 0: 1.069184\n",
      "Loss after iteration 0: 0.989748\n",
      "Loss after iteration 0: 1.047524\n",
      "Loss after iteration 0: 1.105115\n",
      "Loss after iteration 0: 1.084913\n",
      "Loss after iteration 0: 0.991262\n",
      "Loss after iteration 0: 1.027517\n",
      "Loss after iteration 0: 0.983473\n",
      "Loss after iteration 0: 1.048106\n",
      "Loss after iteration 0: 1.080058\n",
      "Loss after iteration 0: 1.075448\n",
      "Loss after iteration 0: 1.017770\n",
      "Loss after iteration 0: 1.102983\n",
      "Loss after iteration 0: 0.943769\n",
      "Loss after iteration 0: 1.091063\n",
      "Loss after iteration 0: 1.109020\n",
      "Loss after iteration 0: 0.935760\n",
      "Loss after iteration 0: 1.074400\n",
      "Loss after iteration 0: 0.997514\n",
      "Loss after iteration 0: 1.054860\n",
      "Loss after iteration 0: 0.877527\n",
      "Loss after iteration 0: 1.096415\n",
      "Loss after iteration 0: 0.835477\n",
      "Loss after iteration 0: 1.094106\n",
      "Loss after iteration 0: 0.883526\n",
      "Loss after iteration 0: 1.102900\n",
      "Loss after iteration 0: 0.843659\n",
      "Loss after iteration 0: 1.053998\n",
      "Loss after iteration 0: 1.085747\n",
      "Loss after iteration 0: 0.984026\n",
      "Loss after iteration 0: 0.870911\n",
      "Loss after iteration 0: 0.653717\n",
      "Loss after iteration 0: 1.181386\n",
      "Loss after iteration 0: 1.033714\n",
      "Loss after iteration 0: 0.866818\n",
      "Loss after iteration 0: 0.968098\n",
      "Loss after iteration 0: 0.823355\n",
      "Loss after iteration 0: 1.135356\n",
      "Loss after iteration 0: 1.113833\n",
      "Loss after iteration 0: 1.099431\n",
      "Loss after iteration 0: 1.047716\n",
      "Loss after iteration 0: 0.973159\n",
      "Loss after iteration 0: 0.963097\n",
      "Loss after iteration 0: 0.818774\n",
      "Loss after iteration 0: 1.119882\n",
      "Loss after iteration 0: 1.108921\n",
      "Loss after iteration 0: 1.031809\n",
      "Loss after iteration 0: 1.048230\n",
      "Loss after iteration 0: 0.924056\n",
      "Loss after iteration 0: 0.874786\n",
      "Loss after iteration 0: 1.147640\n",
      "Loss after iteration 0: 0.924435\n",
      "Loss after iteration 0: 1.014536\n",
      "Loss after iteration 0: 0.660331\n",
      "Loss after iteration 0: 1.137554\n",
      "Loss after iteration 0: 1.100567\n",
      "Loss after iteration 0: 1.006195\n",
      "Loss after iteration 0: 1.107872\n",
      "Loss after iteration 0: 1.042530\n",
      "Loss after iteration 0: 1.087081\n",
      "Loss after iteration 0: 1.014603\n",
      "Loss after iteration 0: 0.899864\n",
      "Loss after iteration 0: 1.019397\n",
      "Loss after iteration 0: 1.086797\n",
      "Loss after iteration 0: 0.656472\n",
      "Loss after iteration 0: 1.097815\n",
      "Loss after iteration 0: 1.081254\n",
      "Loss after iteration 0: 1.097027\n",
      "Loss after iteration 0: 1.023791\n",
      "Loss after iteration 0: 1.072768\n",
      "Loss after iteration 0: 1.046917\n",
      "Loss after iteration 0: 1.070078\n",
      "Loss after iteration 0: 1.089263\n",
      "Loss after iteration 0: 0.930006\n",
      "Loss after iteration 0: 1.068031\n",
      "Loss after iteration 0: 1.015338\n",
      "Loss after iteration 0: 1.045205\n",
      "Loss after iteration 0: 1.091674\n",
      "Loss after iteration 0: 1.028737\n",
      "Loss after iteration 0: 1.029489\n",
      "Loss after iteration 0: 1.087413\n",
      "Loss after iteration 0: 1.086585\n",
      "Loss after iteration 0: 1.065298\n",
      "Loss after iteration 0: 1.017040\n",
      "Loss after iteration 0: 0.981800\n",
      "Loss after iteration 0: 0.905880\n",
      "Loss after iteration 0: 1.091238\n",
      "Loss after iteration 0: 1.108008\n",
      "Loss after iteration 0: 0.941504\n",
      "Loss after iteration 0: 1.061963\n",
      "Loss after iteration 0: 0.992621\n",
      "Loss after iteration 0: 1.083594\n",
      "Loss after iteration 0: 0.989865\n",
      "Loss after iteration 0: 1.040208\n",
      "Loss after iteration 0: 0.605506\n",
      "Loss after iteration 0: 1.122864\n",
      "Loss after iteration 0: 0.974127\n",
      "Loss after iteration 0: 1.006838\n",
      "Loss after iteration 0: 1.115766\n",
      "Loss after iteration 0: 1.090921\n",
      "Loss after iteration 0: 1.105733\n",
      "Loss after iteration 0: 1.083615\n",
      "Loss after iteration 0: 1.091675\n",
      "Loss after iteration 0: 1.083277\n",
      "Loss after iteration 0: 0.843311\n",
      "Loss after iteration 0: 1.014582\n",
      "Loss after iteration 0: 1.117453\n",
      "Loss after iteration 0: 1.039176\n",
      "Loss after iteration 0: 0.876884\n",
      "Loss after iteration 0: 0.990640\n",
      "Loss after iteration 0: 0.822444\n",
      "Loss after iteration 0: 1.148606\n",
      "Loss after iteration 0: 0.997796\n",
      "Loss after iteration 0: 1.079661\n",
      "Loss after iteration 0: 0.875096\n",
      "Loss after iteration 0: 0.891520\n",
      "Loss after iteration 0: 1.085788\n",
      "Loss after iteration 0: 1.068111\n",
      "Loss after iteration 0: 0.958135\n",
      "Loss after iteration 0: 1.018449\n",
      "Loss after iteration 0: 1.011852\n",
      "Loss after iteration 0: 0.764531\n",
      "Loss after iteration 0: 1.073631\n",
      "Loss after iteration 0: 1.083582\n",
      "Loss after iteration 0: 0.938198\n",
      "Loss after iteration 0: 1.120344\n",
      "Loss after iteration 0: 1.035976\n",
      "Loss after iteration 0: 1.083800\n",
      "Loss after iteration 0: 1.039436\n",
      "Loss after iteration 0: 0.990881\n",
      "Loss after iteration 0: 1.084249\n",
      "Loss after iteration 0: 1.081317\n",
      "Loss after iteration 0: 0.996884\n",
      "Loss after iteration 0: 1.003630\n",
      "Loss after iteration 0: 0.976128\n",
      "Loss after iteration 0: 0.977394\n",
      "Loss after iteration 0: 0.919819\n",
      "Loss after iteration 0: 0.868742\n",
      "Loss after iteration 0: 1.105481\n",
      "Loss after iteration 0: 1.081420\n",
      "Loss after iteration 0: 1.044765\n",
      "Loss after iteration 0: 0.971813\n",
      "Loss after iteration 0: 1.116933\n",
      "Loss after iteration 0: 1.010725\n",
      "Loss after iteration 0: 1.021098\n",
      "Loss after iteration 0: 0.967172\n",
      "Loss after iteration 0: 1.044034\n",
      "Loss after iteration 0: 0.693617\n",
      "Loss after iteration 0: 1.051396\n",
      "Loss after iteration 0: 1.064386\n",
      "Loss after iteration 0: 1.032484\n",
      "Loss after iteration 0: 0.842132\n",
      "Loss after iteration 0: 1.088240\n",
      "Loss after iteration 0: 1.037313\n",
      "Loss after iteration 0: 1.122995\n",
      "Loss after iteration 0: 1.045012\n",
      "Loss after iteration 0: 0.872116\n",
      "Loss after iteration 0: 0.887714\n",
      "Loss after iteration 0: 1.098787\n",
      "Loss after iteration 0: 1.080912\n",
      "Loss after iteration 0: 0.860814\n",
      "Loss after iteration 0: 1.090966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.069097\n",
      "Loss after iteration 0: 1.033859\n",
      "Loss after iteration 0: 0.987290\n",
      "Loss after iteration 0: 0.992071\n",
      "Loss after iteration 0: 1.080128\n",
      "Loss after iteration 0: 1.030221\n",
      "Loss after iteration 0: 1.083283\n",
      "Loss after iteration 0: 0.969092\n",
      "Loss after iteration 0: 1.094777\n",
      "Loss after iteration 0: 1.025510\n",
      "Loss after iteration 0: 0.863770\n",
      "Loss after iteration 0: 1.143908\n",
      "Loss after iteration 0: 1.028453\n",
      "Loss after iteration 0: 1.108014\n",
      "Loss after iteration 0: 0.890174\n",
      "Loss after iteration 0: 1.116352\n",
      "Loss after iteration 0: 1.006038\n",
      "Loss after iteration 0: 1.083451\n",
      "Loss after iteration 0: 0.991351\n",
      "Loss after iteration 0: 0.972699\n",
      "Loss after iteration 0: 1.032585\n",
      "Loss after iteration 0: 1.078601\n",
      "Loss after iteration 0: 1.079687\n",
      "Loss after iteration 0: 0.992328\n",
      "Loss after iteration 0: 1.102283\n",
      "Loss after iteration 0: 1.082839\n",
      "Loss after iteration 0: 1.012309\n",
      "Loss after iteration 0: 0.882562\n",
      "Loss after iteration 0: 1.127258\n",
      "Loss after iteration 0: 1.081024\n",
      "Loss after iteration 0: 1.079658\n",
      "Loss after iteration 0: 1.040431\n",
      "Loss after iteration 0: 0.657624\n",
      "Loss after iteration 0: 0.939138\n",
      "Loss after iteration 0: 1.100933\n",
      "Loss after iteration 0: 1.015777\n",
      "Loss after iteration 0: 1.094837\n",
      "Loss after iteration 0: 1.086367\n",
      "Loss after iteration 0: 1.092107\n",
      "Loss after iteration 0: 0.992054\n",
      "Loss after iteration 0: 1.081026\n",
      "Loss after iteration 0: 1.030827\n",
      "Loss after iteration 0: 1.084956\n",
      "Loss after iteration 0: 1.111863\n",
      "Loss after iteration 0: 1.018844\n",
      "Loss after iteration 0: 1.042198\n",
      "Loss after iteration 0: 0.853925\n",
      "Loss after iteration 0: 1.021043\n",
      "Loss after iteration 0: 1.029037\n",
      "Loss after iteration 0: 1.018415\n",
      "Loss after iteration 0: 1.042540\n",
      "Loss after iteration 0: 0.993208\n",
      "Loss after iteration 0: 1.070476\n",
      "Loss after iteration 0: 0.997039\n",
      "Loss after iteration 0: 0.971139\n",
      "Loss after iteration 0: 1.082665\n",
      "Loss after iteration 0: 1.053465\n",
      "Loss after iteration 0: 0.921017\n",
      "Loss after iteration 0: 0.964095\n",
      "Loss after iteration 0: 1.091843\n",
      "Loss after iteration 0: 1.063995\n",
      "Loss after iteration 0: 1.080194\n",
      "Loss after iteration 0: 1.044155\n",
      "Loss after iteration 0: 1.111992\n",
      "Loss after iteration 0: 1.090203\n",
      "Loss after iteration 0: 1.019369\n",
      "Loss after iteration 0: 0.855917\n",
      "Loss after iteration 0: 1.056241\n",
      "Loss after iteration 0: 0.616897\n",
      "Loss after iteration 0: 1.112038\n",
      "Loss after iteration 0: 1.136847\n",
      "Loss after iteration 0: 1.090313\n",
      "Loss after iteration 0: 1.001717\n",
      "Loss after iteration 0: 0.953718\n",
      "Loss after iteration 0: 1.025365\n",
      "Loss after iteration 0: 0.987005\n",
      "Loss after iteration 0: 1.074009\n",
      "Loss after iteration 0: 1.027688\n",
      "Loss after iteration 0: 1.086679\n",
      "Loss after iteration 0: 0.934258\n",
      "Loss after iteration 0: 1.080560\n",
      "Loss after iteration 0: 1.048248\n",
      "Loss after iteration 0: 0.691038\n",
      "Loss after iteration 0: 0.957090\n",
      "Loss after iteration 0: 1.010269\n",
      "Loss after iteration 0: 1.162465\n",
      "Loss after iteration 0: 0.977590\n",
      "Loss after iteration 0: 1.112193\n",
      "Loss after iteration 0: 0.834923\n",
      "Loss after iteration 0: 1.003784\n",
      "Loss after iteration 0: 0.864369\n",
      "Loss after iteration 0: 1.095708\n",
      "Loss after iteration 0: 1.117122\n",
      "Loss after iteration 0: 1.079859\n",
      "Loss after iteration 0: 0.892600\n",
      "Loss after iteration 0: 1.106345\n",
      "Loss after iteration 0: 1.013554\n",
      "Loss after iteration 0: 1.052711\n",
      "Loss after iteration 0: 1.091149\n",
      "Loss after iteration 0: 1.060020\n",
      "Loss after iteration 0: 1.040951\n",
      "Loss after iteration 0: 0.991500\n",
      "Loss after iteration 0: 1.051928\n",
      "Loss after iteration 0: 1.074472\n",
      "Loss after iteration 0: 0.726108\n",
      "Loss after iteration 0: 1.072421\n",
      "Loss after iteration 0: 1.082332\n",
      "Loss after iteration 0: 1.093750\n",
      "Loss after iteration 0: 1.084239\n",
      "Loss after iteration 0: 0.937884\n",
      "Loss after iteration 0: 1.032272\n",
      "Loss after iteration 0: 0.872759\n",
      "Loss after iteration 0: 1.113059\n",
      "Loss after iteration 0: 1.099919\n",
      "Loss after iteration 0: 1.066633\n",
      "Loss after iteration 0: 0.901343\n",
      "Loss after iteration 0: 1.082643\n",
      "Loss after iteration 0: 1.015204\n",
      "Loss after iteration 0: 1.075355\n",
      "Loss after iteration 0: 1.051888\n",
      "Loss after iteration 0: 1.027469\n",
      "Loss after iteration 0: 0.670938\n",
      "Loss after iteration 0: 1.145145\n",
      "Loss after iteration 0: 0.976483\n",
      "Loss after iteration 0: 0.978429\n",
      "Loss after iteration 0: 0.924537\n",
      "Loss after iteration 0: 1.088218\n",
      "Loss after iteration 0: 1.104770\n",
      "Loss after iteration 0: 1.016653\n",
      "Loss after iteration 0: 1.110696\n",
      "Loss after iteration 0: 2.296281\n",
      "Loss after iteration 0: 3.615027\n",
      "Loss after iteration 0: 4.444305\n",
      "Loss after iteration 0: 1.676139\n",
      "Loss after iteration 0: 1.300562\n",
      "Loss after iteration 0: 0.963398\n",
      "Loss after iteration 0: 1.034629\n",
      "Loss after iteration 0: 1.053569\n",
      "Loss after iteration 0: 1.026839\n",
      "Loss after iteration 0: 1.112380\n",
      "Loss after iteration 0: 1.086076\n",
      "Loss after iteration 0: 1.085499\n",
      "Loss after iteration 0: 1.047005\n",
      "Loss after iteration 0: 1.089959\n",
      "Loss after iteration 0: 1.047655\n",
      "Loss after iteration 0: 0.980804\n",
      "Loss after iteration 0: 1.050030\n",
      "Loss after iteration 0: 1.066087\n",
      "Loss after iteration 0: 1.012336\n",
      "Loss after iteration 0: 1.097214\n",
      "Loss after iteration 0: 1.061311\n",
      "Loss after iteration 0: 1.085218\n",
      "Loss after iteration 0: 0.997847\n",
      "Loss after iteration 0: 0.993689\n",
      "Loss after iteration 0: 1.051264\n",
      "Loss after iteration 0: 1.003691\n",
      "Loss after iteration 0: 1.054963\n",
      "Loss after iteration 0: 1.053923\n",
      "Loss after iteration 0: 1.060079\n",
      "Loss after iteration 0: 1.085407\n",
      "Loss after iteration 0: 1.059829\n",
      "Loss after iteration 0: 1.014137\n",
      "Loss after iteration 0: 1.062636\n",
      "Loss after iteration 0: 1.085513\n",
      "Loss after iteration 0: 1.098919\n",
      "Loss after iteration 0: 1.011107\n",
      "Loss after iteration 0: 1.063036\n",
      "Loss after iteration 0: 1.061540\n",
      "Loss after iteration 0: 1.061636\n",
      "Loss after iteration 0: 1.010371\n",
      "Loss after iteration 0: 1.014324\n",
      "Loss after iteration 0: 1.006276\n",
      "Loss after iteration 0: 0.972883\n",
      "Loss after iteration 0: 1.095100\n",
      "Loss after iteration 0: 0.982289\n",
      "Loss after iteration 0: 1.086390\n",
      "Loss after iteration 0: 1.060046\n",
      "Loss after iteration 0: 0.930768\n",
      "Loss after iteration 0: 1.055438\n",
      "Loss after iteration 0: 1.011984\n",
      "Loss after iteration 0: 1.060319\n",
      "Loss after iteration 0: 1.049186\n",
      "Loss after iteration 0: 0.983191\n",
      "Loss after iteration 0: 1.009769\n",
      "Loss after iteration 0: 1.051103\n",
      "Loss after iteration 0: 1.013959\n",
      "Loss after iteration 0: 1.087176\n",
      "Loss after iteration 0: 1.085263\n",
      "Loss after iteration 0: 1.003328\n",
      "Loss after iteration 0: 1.090833\n",
      "Loss after iteration 0: 1.098804\n",
      "Loss after iteration 0: 1.061016\n",
      "Loss after iteration 0: 1.060604\n",
      "Loss after iteration 0: 1.012384\n",
      "Loss after iteration 0: 1.085485\n",
      "Loss after iteration 0: 1.010009\n",
      "Loss after iteration 0: 1.061657\n",
      "Loss after iteration 0: 1.062110\n",
      "Loss after iteration 0: 1.098892\n",
      "Loss after iteration 0: 1.002240\n",
      "Loss after iteration 0: 1.092539\n",
      "Loss after iteration 0: 1.085974\n",
      "Loss after iteration 0: 1.047557\n",
      "Loss after iteration 0: 0.997054\n",
      "Loss after iteration 0: 1.016792\n",
      "Loss after iteration 0: 0.995263\n",
      "Loss after iteration 0: 0.904803\n",
      "Loss after iteration 0: 1.053407\n",
      "Loss after iteration 0: 0.640507\n",
      "Loss after iteration 0: 1.089342\n",
      "Loss after iteration 0: 0.908761\n",
      "Loss after iteration 0: 0.991105\n",
      "Loss after iteration 0: 1.081840\n",
      "Loss after iteration 0: 0.999638\n",
      "Loss after iteration 0: 0.900682\n",
      "Loss after iteration 0: 1.113292\n",
      "Loss after iteration 0: 1.047102\n",
      "Loss after iteration 0: 0.828227\n",
      "Loss after iteration 0: 0.985555\n",
      "Loss after iteration 0: 0.864942\n",
      "Loss after iteration 0: 1.052242\n",
      "Loss after iteration 0: 1.086725\n",
      "Loss after iteration 0: 1.056970\n",
      "Loss after iteration 0: 0.925411\n",
      "Loss after iteration 0: 1.074980\n",
      "Loss after iteration 0: 0.979216\n",
      "Loss after iteration 0: 1.092098\n",
      "Loss after iteration 0: 1.098720\n",
      "Loss after iteration 0: 1.085633\n",
      "Loss after iteration 0: 1.004066\n",
      "Loss after iteration 0: 1.019699\n",
      "Loss after iteration 0: 1.061281\n",
      "Loss after iteration 0: 1.086679\n",
      "Loss after iteration 0: 1.098799\n",
      "Loss after iteration 0: 1.046272\n",
      "Loss after iteration 0: 0.937781\n",
      "Loss after iteration 0: 1.089166\n",
      "Loss after iteration 0: 1.086018\n",
      "Loss after iteration 0: 1.010405\n",
      "Loss after iteration 0: 0.982599\n",
      "Loss after iteration 0: 1.043763\n",
      "Loss after iteration 0: 1.014560\n",
      "Loss after iteration 0: 1.061976\n",
      "Loss after iteration 0: 1.085970\n",
      "Loss after iteration 0: 1.085974\n",
      "Loss after iteration 0: 0.917951\n",
      "Loss after iteration 0: 1.109889\n",
      "Loss after iteration 0: 1.010010\n",
      "Loss after iteration 0: 1.048400\n",
      "Loss after iteration 0: 1.006289\n",
      "Loss after iteration 0: 0.988174\n",
      "Loss after iteration 0: 1.089135\n",
      "Loss after iteration 0: 0.918586\n",
      "Loss after iteration 0: 0.970479\n",
      "Loss after iteration 0: 1.026596\n",
      "Loss after iteration 0: 1.049532\n",
      "Loss after iteration 0: 1.047562\n",
      "Loss after iteration 0: 1.060214\n",
      "Loss after iteration 0: 1.061381\n",
      "Loss after iteration 0: 1.012895\n",
      "Loss after iteration 0: 1.087123\n",
      "Loss after iteration 0: 0.886050\n",
      "Loss after iteration 0: 1.096548\n",
      "Loss after iteration 0: 0.980598\n",
      "Loss after iteration 0: 1.102402\n",
      "Loss after iteration 0: 1.085834\n",
      "Loss after iteration 0: 0.980356\n",
      "Loss after iteration 0: 1.091415\n",
      "Loss after iteration 0: 1.011786\n",
      "Loss after iteration 0: 1.085273\n",
      "Loss after iteration 0: 1.086685\n",
      "Loss after iteration 0: 0.999635\n",
      "Loss after iteration 0: 1.091995\n",
      "Loss after iteration 0: 1.010743\n",
      "Loss after iteration 0: 1.060003\n",
      "Loss after iteration 0: 1.064058\n",
      "Loss after iteration 0: 1.057386\n",
      "Loss after iteration 0: 1.087714\n",
      "Loss after iteration 0: 1.045662\n",
      "Loss after iteration 0: 1.049447\n",
      "Loss after iteration 0: 1.086928\n",
      "Loss after iteration 0: 1.061573\n",
      "Loss after iteration 0: 1.061671\n",
      "Loss after iteration 0: 0.934341\n",
      "Loss after iteration 0: 1.062434\n",
      "Loss after iteration 0: 1.087169\n",
      "Loss after iteration 0: 1.085730\n",
      "Loss after iteration 0: 0.923873\n",
      "Loss after iteration 0: 1.077676\n",
      "Loss after iteration 0: 1.044921\n",
      "Loss after iteration 0: 0.770103\n",
      "Loss after iteration 0: 1.058129\n",
      "Loss after iteration 0: 1.066243\n",
      "Loss after iteration 0: 1.061532\n",
      "Loss after iteration 0: 1.044997\n",
      "Loss after iteration 0: 1.088583\n",
      "Loss after iteration 0: 1.086109\n",
      "Loss after iteration 0: 1.085834\n",
      "Loss after iteration 0: 1.047419\n",
      "Loss after iteration 0: 1.048234\n",
      "Loss after iteration 0: 1.087381\n",
      "Loss after iteration 0: 1.047177\n",
      "Loss after iteration 0: 1.009708\n",
      "Loss after iteration 0: 1.044049\n",
      "Loss after iteration 0: 1.062984\n",
      "Loss after iteration 0: 1.099615\n",
      "Loss after iteration 0: 1.046524\n",
      "Loss after iteration 0: 1.012521\n",
      "Loss after iteration 0: 0.934451\n",
      "Loss after iteration 0: 1.052440\n",
      "Loss after iteration 0: 0.983181\n",
      "Loss after iteration 0: 0.940984\n",
      "Loss after iteration 0: 1.013957\n",
      "Loss after iteration 0: 1.013998\n",
      "Loss after iteration 0: 1.058901\n",
      "Loss after iteration 0: 0.743886\n",
      "Loss after iteration 0: 1.067837\n",
      "Loss after iteration 0: 1.061718\n",
      "Loss after iteration 0: 0.909373\n",
      "Loss after iteration 0: 1.004735\n",
      "Loss after iteration 0: 1.085556\n",
      "Loss after iteration 0: 1.056136\n",
      "Loss after iteration 0: 1.085842\n",
      "Loss after iteration 0: 1.086564\n",
      "Loss after iteration 0: 1.012164\n",
      "Loss after iteration 0: 1.086842\n",
      "Loss after iteration 0: 0.978290\n",
      "Loss after iteration 0: 1.087321\n",
      "Loss after iteration 0: 1.014196\n",
      "Loss after iteration 0: 0.981655\n",
      "Loss after iteration 0: 1.102803\n",
      "Loss after iteration 0: 1.011960\n",
      "Loss after iteration 0: 1.055985\n",
      "Loss after iteration 0: 1.016687\n",
      "Loss after iteration 0: 0.932001\n",
      "Loss after iteration 0: 1.054688\n",
      "Loss after iteration 0: 1.009798\n",
      "Loss after iteration 0: 1.100112\n",
      "Loss after iteration 0: 1.073077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.548827\n",
      "Loss after iteration 0: 1.152364\n",
      "Loss after iteration 0: 1.719223\n",
      "Loss after iteration 0: 1.150926\n",
      "Loss after iteration 0: 1.374045\n",
      "Loss after iteration 0: 1.267933\n",
      "Loss after iteration 0: 1.094693\n",
      "Loss after iteration 0: 1.093733\n",
      "Loss after iteration 0: 1.079807\n",
      "Loss after iteration 0: 1.221232\n",
      "Loss after iteration 0: 1.196634\n",
      "Loss after iteration 0: 1.180083\n",
      "Loss after iteration 0: 1.082463\n",
      "Loss after iteration 0: 1.128235\n",
      "Loss after iteration 0: 1.114035\n",
      "Loss after iteration 0: 1.126828\n",
      "Loss after iteration 0: 1.134093\n",
      "Loss after iteration 0: 1.109282\n",
      "Loss after iteration 0: 1.095049\n",
      "Loss after iteration 0: 1.110274\n",
      "Loss after iteration 0: 1.236029\n",
      "Loss after iteration 0: 1.146087\n",
      "Loss after iteration 0: 1.207606\n",
      "Loss after iteration 0: 1.052651\n",
      "Loss after iteration 0: 1.180192\n",
      "Loss after iteration 0: 1.189899\n",
      "Loss after iteration 0: 1.055681\n",
      "Loss after iteration 0: 1.095291\n",
      "Loss after iteration 0: 1.095548\n",
      "Loss after iteration 0: 1.086695\n",
      "Loss after iteration 0: 1.088560\n",
      "Loss after iteration 0: 1.106424\n",
      "Loss after iteration 0: 1.175393\n",
      "Loss after iteration 0: 1.243373\n",
      "Loss after iteration 0: 1.097151\n",
      "Loss after iteration 0: 1.113180\n",
      "Loss after iteration 0: 1.096242\n",
      "Loss after iteration 0: 1.059166\n",
      "Loss after iteration 0: 1.142574\n",
      "Loss after iteration 0: 1.112661\n",
      "Loss after iteration 0: 1.059212\n",
      "Loss after iteration 0: 1.090639\n",
      "Loss after iteration 0: 1.065742\n",
      "Loss after iteration 0: 1.032321\n",
      "Loss after iteration 0: 1.057512\n",
      "Loss after iteration 0: 1.123234\n",
      "Loss after iteration 0: 1.137403\n",
      "Loss after iteration 0: 1.184120\n",
      "Loss after iteration 0: 1.181978\n",
      "Loss after iteration 0: 1.102441\n",
      "Loss after iteration 0: 1.114197\n",
      "Loss after iteration 0: 1.114884\n",
      "Loss after iteration 0: 1.108694\n",
      "Loss after iteration 0: 1.231295\n",
      "Loss after iteration 0: 1.053439\n",
      "Loss after iteration 0: 1.235498\n",
      "Loss after iteration 0: 1.169480\n",
      "Loss after iteration 0: 1.078344\n",
      "Loss after iteration 0: 1.097691\n",
      "Loss after iteration 0: 1.134898\n",
      "Loss after iteration 0: 1.087189\n",
      "Loss after iteration 0: 0.991772\n",
      "Loss after iteration 0: 1.146673\n",
      "Loss after iteration 0: 1.113003\n",
      "Loss after iteration 0: 1.089561\n",
      "Loss after iteration 0: 1.049761\n",
      "Loss after iteration 0: 1.046207\n",
      "Loss after iteration 0: 1.122170\n",
      "Loss after iteration 0: 1.106423\n",
      "Loss after iteration 0: 1.076566\n",
      "Loss after iteration 0: 1.073992\n",
      "Loss after iteration 0: 1.141397\n",
      "Loss after iteration 0: 1.136776\n",
      "Loss after iteration 0: 1.087077\n",
      "Loss after iteration 0: 1.096321\n",
      "Loss after iteration 0: 1.102456\n",
      "Loss after iteration 0: 1.078506\n",
      "Loss after iteration 0: 1.185797\n",
      "Loss after iteration 0: 1.046657\n",
      "Loss after iteration 0: 1.069684\n",
      "Loss after iteration 0: 0.956505\n",
      "Loss after iteration 0: 1.172975\n",
      "Loss after iteration 0: 1.129288\n",
      "Loss after iteration 0: 1.094158\n",
      "Loss after iteration 0: 1.105775\n",
      "Loss after iteration 0: 1.172623\n",
      "Loss after iteration 0: 1.105107\n",
      "Loss after iteration 0: 1.191605\n",
      "Loss after iteration 0: 1.087767\n",
      "Loss after iteration 0: 1.183915\n",
      "Loss after iteration 0: 1.088182\n",
      "Loss after iteration 0: 1.132171\n",
      "Loss after iteration 0: 1.030043\n",
      "Loss after iteration 0: 1.225409\n",
      "Loss after iteration 0: 1.112329\n",
      "Loss after iteration 0: 1.205534\n",
      "Loss after iteration 0: 1.160502\n",
      "Loss after iteration 0: 1.133703\n",
      "Loss after iteration 0: 1.097378\n",
      "Loss after iteration 0: 1.113195\n",
      "Loss after iteration 0: 1.118257\n",
      "Loss after iteration 0: 1.135392\n",
      "Loss after iteration 0: 1.100933\n",
      "Loss after iteration 0: 1.074694\n",
      "Loss after iteration 0: 1.094027\n",
      "Found hyperparameter value: Batch size 8\n",
      "Best f1-score on validation set 0.5035971223021583\n"
     ]
    }
   ],
   "source": [
    "possible_hp_values = [8,16,32]\n",
    "best_hp_value = None\n",
    "best_f1_score = -np.infty\n",
    "for hp_value in possible_hp_values:\n",
    "    # 1. create a baseline classifier object with hp_value specified\n",
    "    nnn = SimpleNN()\n",
    "    nnn.add(activation=\"tanh\",input_dim=8, hidden_dim=16, output_dim=3, print_loss=True)\n",
    "\n",
    "    # 2. learn the classifier on the training set\n",
    "    nnn.train(uci_features_train, uci_labels_train, epoches=2000, reg_value=0.01, epsilon=0.01, batch_size=hp_value)\n",
    "\n",
    "    # 3. evaluate the model on the validation set\n",
    "    prediction = nn.infer(uci_features_valid)\n",
    "\n",
    "    \n",
    "    confusion = confusion_matrix(testy, predy)\n",
    "    confusion = np.array(confusion)\n",
    "    f1_score = F1_score(confusion)\n",
    "    \n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        best_hp_value = hp_value\n",
    "\n",
    "print(\"Found hyperparameter value: Batch size\", best_hp_value)\n",
    "print(\"Best f1-score on validation set\", best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7f53eb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hp_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb80bc2",
   "metadata": {},
   "source": [
    "__best_hp_value : used from above, i.e. batch size__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e225561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 3.637815\n",
      "Loss after iteration 0: 1.910060\n",
      "Loss after iteration 0: 4.183887\n",
      "Loss after iteration 0: 3.309771\n",
      "Loss after iteration 0: 3.656066\n",
      "Loss after iteration 0: 2.397539\n",
      "Loss after iteration 0: 2.634979\n",
      "Loss after iteration 0: 2.065391\n",
      "Loss after iteration 0: 1.488796\n",
      "Loss after iteration 0: 0.912948\n",
      "Loss after iteration 0: 1.038738\n",
      "Loss after iteration 0: 0.770882\n",
      "Loss after iteration 0: 1.020170\n",
      "Loss after iteration 0: 1.020745\n",
      "Loss after iteration 0: 1.097246\n",
      "Loss after iteration 0: 1.013267\n",
      "Loss after iteration 0: 1.092186\n",
      "Loss after iteration 0: 0.635765\n",
      "Loss after iteration 0: 1.031634\n",
      "Loss after iteration 0: 1.041832\n",
      "Loss after iteration 0: 1.080751\n",
      "Loss after iteration 0: 1.079451\n",
      "Loss after iteration 0: 1.079147\n",
      "Loss after iteration 0: 0.998578\n",
      "Loss after iteration 0: 0.775309\n",
      "Loss after iteration 0: 0.959161\n",
      "Loss after iteration 0: 1.088039\n",
      "Loss after iteration 0: 1.055747\n",
      "Loss after iteration 0: 0.975239\n",
      "Loss after iteration 0: 1.105002\n",
      "Loss after iteration 0: 1.037454\n",
      "Loss after iteration 0: 0.938885\n",
      "Loss after iteration 0: 0.848282\n",
      "Loss after iteration 0: 1.113089\n",
      "Loss after iteration 0: 0.971720\n",
      "Loss after iteration 0: 1.128043\n",
      "Loss after iteration 0: 1.016466\n",
      "Loss after iteration 0: 1.056370\n",
      "Loss after iteration 0: 0.905724\n",
      "Loss after iteration 0: 1.131764\n",
      "Loss after iteration 0: 1.000491\n",
      "Loss after iteration 0: 1.063340\n",
      "Loss after iteration 0: 1.042275\n",
      "Loss after iteration 0: 1.092281\n",
      "Loss after iteration 0: 0.991752\n",
      "Loss after iteration 0: 0.886505\n",
      "Loss after iteration 0: 0.988917\n",
      "Loss after iteration 0: 1.091930\n",
      "Loss after iteration 0: 1.038784\n",
      "Loss after iteration 0: 1.012391\n",
      "Loss after iteration 0: 0.976639\n",
      "Loss after iteration 0: 0.886764\n",
      "Loss after iteration 0: 0.974346\n",
      "Loss after iteration 0: 1.107530\n",
      "Loss after iteration 0: 1.087314\n",
      "Loss after iteration 0: 1.096992\n",
      "Loss after iteration 0: 1.084334\n",
      "Loss after iteration 0: 0.921470\n",
      "Loss after iteration 0: 1.089426\n",
      "Loss after iteration 0: 1.009150\n",
      "Loss after iteration 0: 1.059966\n",
      "Loss after iteration 0: 1.009613\n",
      "Loss after iteration 0: 1.010876\n",
      "Loss after iteration 0: 1.002660\n",
      "Loss after iteration 0: 1.056959\n",
      "Loss after iteration 0: 1.089226\n",
      "Loss after iteration 0: 1.085551\n",
      "Loss after iteration 0: 1.096272\n",
      "Loss after iteration 0: 1.084060\n",
      "Loss after iteration 0: 1.056580\n",
      "Loss after iteration 0: 1.084397\n",
      "Loss after iteration 0: 0.889340\n",
      "Loss after iteration 0: 1.083665\n",
      "Loss after iteration 0: 0.939350\n",
      "Loss after iteration 0: 1.081646\n",
      "Loss after iteration 0: 1.049706\n",
      "Loss after iteration 0: 1.090814\n",
      "Loss after iteration 0: 1.082790\n",
      "Loss after iteration 0: 0.809695\n",
      "Loss after iteration 0: 1.092307\n",
      "Loss after iteration 0: 1.121658\n",
      "Loss after iteration 0: 1.024109\n",
      "Loss after iteration 0: 1.013875\n",
      "Loss after iteration 0: 0.958526\n",
      "Loss after iteration 0: 1.085741\n",
      "Loss after iteration 0: 0.670507\n",
      "Loss after iteration 0: 0.956496\n",
      "Loss after iteration 0: 1.168676\n",
      "Loss after iteration 0: 1.001360\n",
      "Loss after iteration 0: 0.982883\n",
      "Loss after iteration 0: 1.083069\n",
      "Loss after iteration 0: 1.028335\n",
      "Loss after iteration 0: 1.033841\n",
      "Loss after iteration 0: 1.008733\n",
      "Loss after iteration 0: 0.850494\n",
      "Loss after iteration 0: 0.962766\n",
      "Loss after iteration 0: 1.157098\n",
      "Loss after iteration 0: 1.041960\n",
      "Loss after iteration 0: 1.005261\n",
      "Loss after iteration 0: 0.975313\n",
      "Loss after iteration 0: 1.112799\n",
      "Loss after iteration 0: 1.050585\n",
      "Loss after iteration 0: 0.875970\n",
      "Loss after iteration 0: 1.119337\n",
      "Loss after iteration 0: 0.989786\n",
      "Loss after iteration 0: 1.028001\n",
      "Loss after iteration 0: 0.986099\n",
      "Loss after iteration 0: 1.092750\n",
      "Loss after iteration 0: 1.121502\n",
      "Loss after iteration 0: 1.047963\n",
      "Loss after iteration 0: 1.025376\n",
      "Loss after iteration 0: 0.819121\n",
      "Loss after iteration 0: 1.115867\n",
      "Loss after iteration 0: 1.091381\n",
      "Loss after iteration 0: 1.092416\n",
      "Loss after iteration 0: 1.086361\n",
      "Loss after iteration 0: 1.042376\n",
      "Loss after iteration 0: 0.990090\n",
      "Loss after iteration 0: 1.080988\n",
      "Loss after iteration 0: 1.049343\n",
      "Loss after iteration 0: 1.084397\n",
      "Loss after iteration 0: 1.088501\n",
      "Loss after iteration 0: 0.994984\n",
      "Loss after iteration 0: 1.116230\n",
      "Loss after iteration 0: 1.080865\n",
      "Loss after iteration 0: 0.989525\n",
      "Loss after iteration 0: 1.069184\n",
      "Loss after iteration 0: 0.989748\n",
      "Loss after iteration 0: 1.047524\n",
      "Loss after iteration 0: 1.105115\n",
      "Loss after iteration 0: 1.084913\n",
      "Loss after iteration 0: 0.991262\n",
      "Loss after iteration 0: 1.027517\n",
      "Loss after iteration 0: 0.983473\n",
      "Loss after iteration 0: 1.048106\n",
      "Loss after iteration 0: 1.080058\n",
      "Loss after iteration 0: 1.075448\n",
      "Loss after iteration 0: 1.017770\n",
      "Loss after iteration 0: 1.102983\n",
      "Loss after iteration 0: 0.943769\n",
      "Loss after iteration 0: 1.091063\n",
      "Loss after iteration 0: 1.109020\n",
      "Loss after iteration 0: 0.935760\n",
      "Loss after iteration 0: 1.074400\n",
      "Loss after iteration 0: 0.997514\n",
      "Loss after iteration 0: 1.054860\n",
      "Loss after iteration 0: 0.877527\n",
      "Loss after iteration 0: 1.096415\n",
      "Loss after iteration 0: 0.835477\n",
      "Loss after iteration 0: 1.094106\n",
      "Loss after iteration 0: 0.883526\n",
      "Loss after iteration 0: 1.102900\n",
      "Loss after iteration 0: 0.843659\n",
      "Loss after iteration 0: 1.053998\n",
      "Loss after iteration 0: 1.085747\n",
      "Loss after iteration 0: 0.984026\n",
      "Loss after iteration 0: 0.870911\n",
      "Loss after iteration 0: 0.653717\n",
      "Loss after iteration 0: 1.181386\n",
      "Loss after iteration 0: 1.033714\n",
      "Loss after iteration 0: 0.866818\n",
      "Loss after iteration 0: 0.968098\n",
      "Loss after iteration 0: 0.823355\n",
      "Loss after iteration 0: 1.135356\n",
      "Loss after iteration 0: 1.113833\n",
      "Loss after iteration 0: 1.099431\n",
      "Loss after iteration 0: 1.047716\n",
      "Loss after iteration 0: 0.973159\n",
      "Loss after iteration 0: 0.963097\n",
      "Loss after iteration 0: 0.818774\n",
      "Loss after iteration 0: 1.119882\n",
      "Loss after iteration 0: 1.108921\n",
      "Loss after iteration 0: 1.031809\n",
      "Loss after iteration 0: 1.048230\n",
      "Loss after iteration 0: 0.924056\n",
      "Loss after iteration 0: 0.874786\n",
      "Loss after iteration 0: 1.147640\n",
      "Loss after iteration 0: 0.924435\n",
      "Loss after iteration 0: 1.014536\n",
      "Loss after iteration 0: 0.660331\n",
      "Loss after iteration 0: 1.137554\n",
      "Loss after iteration 0: 1.100567\n",
      "Loss after iteration 0: 1.006195\n",
      "Loss after iteration 0: 1.107872\n",
      "Loss after iteration 0: 1.042530\n",
      "Loss after iteration 0: 1.087081\n",
      "Loss after iteration 0: 1.014603\n",
      "Loss after iteration 0: 0.899864\n",
      "Loss after iteration 0: 1.019397\n",
      "Loss after iteration 0: 1.086797\n",
      "Loss after iteration 0: 0.656472\n",
      "Loss after iteration 0: 1.097815\n",
      "Loss after iteration 0: 1.081254\n",
      "Loss after iteration 0: 1.097027\n",
      "Loss after iteration 0: 1.023791\n",
      "Loss after iteration 0: 1.072768\n",
      "Loss after iteration 0: 1.046917\n",
      "Loss after iteration 0: 1.070078\n",
      "Loss after iteration 0: 1.089263\n",
      "Loss after iteration 0: 0.930006\n",
      "Loss after iteration 0: 1.068031\n",
      "Loss after iteration 0: 1.015338\n",
      "Loss after iteration 0: 1.045205\n",
      "Loss after iteration 0: 1.091674\n",
      "Loss after iteration 0: 1.028737\n",
      "Loss after iteration 0: 1.029489\n",
      "Loss after iteration 0: 1.087413\n",
      "Loss after iteration 0: 1.086585\n",
      "Loss after iteration 0: 1.065298\n",
      "Loss after iteration 0: 1.017040\n",
      "Loss after iteration 0: 0.981800\n",
      "Loss after iteration 0: 0.905880\n",
      "Loss after iteration 0: 1.091238\n",
      "Loss after iteration 0: 1.108008\n",
      "Loss after iteration 0: 0.941504\n",
      "Loss after iteration 0: 1.061963\n",
      "Loss after iteration 0: 0.992621\n",
      "Loss after iteration 0: 1.083594\n",
      "Loss after iteration 0: 0.989865\n",
      "Loss after iteration 0: 1.040208\n",
      "Loss after iteration 0: 0.605506\n",
      "Loss after iteration 0: 1.122864\n",
      "Loss after iteration 0: 0.974127\n",
      "Loss after iteration 0: 1.006838\n",
      "Loss after iteration 0: 1.115766\n",
      "Loss after iteration 0: 1.090921\n",
      "Loss after iteration 0: 1.105733\n",
      "Loss after iteration 0: 1.083615\n",
      "Loss after iteration 0: 1.091675\n",
      "Loss after iteration 0: 1.083277\n",
      "Loss after iteration 0: 0.843311\n",
      "Loss after iteration 0: 1.014582\n",
      "Loss after iteration 0: 1.117453\n",
      "Loss after iteration 0: 1.039176\n",
      "Loss after iteration 0: 0.876884\n",
      "Loss after iteration 0: 0.990640\n",
      "Loss after iteration 0: 0.822444\n",
      "Loss after iteration 0: 1.148606\n",
      "Loss after iteration 0: 0.997796\n",
      "Loss after iteration 0: 1.079661\n",
      "Loss after iteration 0: 0.875096\n",
      "Loss after iteration 0: 0.891520\n",
      "Loss after iteration 0: 1.085788\n",
      "Loss after iteration 0: 1.068111\n",
      "Loss after iteration 0: 0.958135\n",
      "Loss after iteration 0: 1.018449\n",
      "Loss after iteration 0: 1.011852\n",
      "Loss after iteration 0: 0.764531\n",
      "Loss after iteration 0: 1.073631\n",
      "Loss after iteration 0: 1.083582\n",
      "Loss after iteration 0: 0.938198\n",
      "Loss after iteration 0: 1.120344\n",
      "Loss after iteration 0: 1.035976\n",
      "Loss after iteration 0: 1.083800\n",
      "Loss after iteration 0: 1.039436\n",
      "Loss after iteration 0: 0.990881\n",
      "Loss after iteration 0: 1.084249\n",
      "Loss after iteration 0: 1.081317\n",
      "Loss after iteration 0: 0.996884\n",
      "Loss after iteration 0: 1.003630\n",
      "Loss after iteration 0: 0.976128\n",
      "Loss after iteration 0: 0.977394\n",
      "Loss after iteration 0: 0.919819\n",
      "Loss after iteration 0: 0.868742\n",
      "Loss after iteration 0: 1.105481\n",
      "Loss after iteration 0: 1.081420\n",
      "Loss after iteration 0: 1.044765\n",
      "Loss after iteration 0: 0.971813\n",
      "Loss after iteration 0: 1.116933\n",
      "Loss after iteration 0: 1.010725\n",
      "Loss after iteration 0: 1.021098\n",
      "Loss after iteration 0: 0.967172\n",
      "Loss after iteration 0: 1.044034\n",
      "Loss after iteration 0: 0.693617\n",
      "Loss after iteration 0: 1.051396\n",
      "Loss after iteration 0: 1.064386\n",
      "Loss after iteration 0: 1.032484\n",
      "Loss after iteration 0: 0.842132\n",
      "Loss after iteration 0: 1.088240\n",
      "Loss after iteration 0: 1.037313\n",
      "Loss after iteration 0: 1.122995\n",
      "Loss after iteration 0: 1.045012\n",
      "Loss after iteration 0: 0.872116\n",
      "Loss after iteration 0: 0.887714\n",
      "Loss after iteration 0: 1.098787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.080912\n",
      "Loss after iteration 0: 0.860814\n",
      "Loss after iteration 0: 1.090966\n",
      "Loss after iteration 0: 1.069097\n",
      "Loss after iteration 0: 1.033859\n",
      "Loss after iteration 0: 0.987290\n",
      "Loss after iteration 0: 0.992071\n",
      "Loss after iteration 0: 1.080128\n",
      "Loss after iteration 0: 1.030221\n",
      "Loss after iteration 0: 1.083283\n",
      "Loss after iteration 0: 0.969092\n",
      "Loss after iteration 0: 1.094777\n",
      "Loss after iteration 0: 1.025510\n",
      "Loss after iteration 0: 0.863770\n",
      "Loss after iteration 0: 1.143908\n",
      "Loss after iteration 0: 1.028453\n",
      "Loss after iteration 0: 1.108014\n",
      "Loss after iteration 0: 0.890174\n",
      "Loss after iteration 0: 1.116352\n",
      "Loss after iteration 0: 1.006038\n",
      "Loss after iteration 0: 1.083451\n",
      "Loss after iteration 0: 0.991351\n",
      "Loss after iteration 0: 0.972699\n",
      "Loss after iteration 0: 1.032585\n",
      "Loss after iteration 0: 1.078601\n",
      "Loss after iteration 0: 1.079687\n",
      "Loss after iteration 0: 0.992328\n",
      "Loss after iteration 0: 1.102283\n",
      "Loss after iteration 0: 1.082839\n",
      "Loss after iteration 0: 1.012309\n",
      "Loss after iteration 0: 0.882562\n",
      "Loss after iteration 0: 1.127258\n",
      "Loss after iteration 0: 1.081024\n",
      "Loss after iteration 0: 1.079658\n",
      "Loss after iteration 0: 1.040431\n",
      "Loss after iteration 0: 0.657624\n",
      "Loss after iteration 0: 0.939138\n",
      "Loss after iteration 0: 1.100933\n",
      "Loss after iteration 0: 1.015777\n",
      "Loss after iteration 0: 1.094837\n",
      "Loss after iteration 0: 1.086367\n",
      "Loss after iteration 0: 1.092107\n",
      "Loss after iteration 0: 0.992054\n",
      "Loss after iteration 0: 1.081026\n",
      "Loss after iteration 0: 1.030827\n",
      "Loss after iteration 0: 1.084956\n",
      "Loss after iteration 0: 1.111863\n",
      "Loss after iteration 0: 1.018844\n",
      "Loss after iteration 0: 1.042198\n",
      "Loss after iteration 0: 0.853925\n",
      "Loss after iteration 0: 1.021043\n",
      "Loss after iteration 0: 1.029037\n",
      "Loss after iteration 0: 1.018415\n",
      "Loss after iteration 0: 1.042540\n",
      "Loss after iteration 0: 0.993208\n",
      "Loss after iteration 0: 1.070476\n",
      "Loss after iteration 0: 0.997039\n",
      "Loss after iteration 0: 0.971139\n",
      "Loss after iteration 0: 1.082665\n",
      "Loss after iteration 0: 1.053465\n",
      "Loss after iteration 0: 0.921017\n",
      "Loss after iteration 0: 0.964095\n",
      "Loss after iteration 0: 1.091843\n",
      "Loss after iteration 0: 1.063995\n",
      "Loss after iteration 0: 1.080194\n",
      "Loss after iteration 0: 1.044155\n",
      "Loss after iteration 0: 1.111992\n",
      "Loss after iteration 0: 1.090203\n",
      "Loss after iteration 0: 1.019369\n",
      "Loss after iteration 0: 0.855917\n",
      "Loss after iteration 0: 1.056241\n",
      "Loss after iteration 0: 0.616897\n",
      "Loss after iteration 0: 1.112038\n",
      "Loss after iteration 0: 1.136847\n",
      "Loss after iteration 0: 1.090313\n",
      "Loss after iteration 0: 1.001717\n",
      "Loss after iteration 0: 0.953718\n",
      "Loss after iteration 0: 1.025365\n",
      "Loss after iteration 0: 0.987005\n",
      "Loss after iteration 0: 1.074009\n",
      "Loss after iteration 0: 1.027688\n",
      "Loss after iteration 0: 1.086679\n",
      "Loss after iteration 0: 0.934258\n",
      "Loss after iteration 0: 1.080560\n",
      "Loss after iteration 0: 1.048248\n",
      "Loss after iteration 0: 0.691038\n",
      "Loss after iteration 0: 0.957090\n",
      "Loss after iteration 0: 1.010269\n",
      "Loss after iteration 0: 1.162465\n",
      "Loss after iteration 0: 0.977590\n",
      "Loss after iteration 0: 1.112193\n",
      "Loss after iteration 0: 0.834923\n",
      "Loss after iteration 0: 1.003784\n",
      "Loss after iteration 0: 0.864369\n",
      "Loss after iteration 0: 1.095708\n",
      "Loss after iteration 0: 1.117122\n",
      "Loss after iteration 0: 1.079859\n",
      "Loss after iteration 0: 0.892600\n",
      "Loss after iteration 0: 1.106345\n",
      "Loss after iteration 0: 1.013554\n",
      "Loss after iteration 0: 1.052711\n",
      "Loss after iteration 0: 1.091149\n",
      "Loss after iteration 0: 1.060020\n",
      "Loss after iteration 0: 1.040951\n",
      "Loss after iteration 0: 0.991500\n",
      "Loss after iteration 0: 1.051928\n",
      "Loss after iteration 0: 1.074472\n",
      "Loss after iteration 0: 0.726108\n",
      "Loss after iteration 0: 1.072421\n",
      "Loss after iteration 0: 1.082332\n",
      "Loss after iteration 0: 1.093750\n",
      "Loss after iteration 0: 1.084239\n",
      "Loss after iteration 0: 0.937884\n",
      "Loss after iteration 0: 1.032272\n",
      "Loss after iteration 0: 0.872759\n",
      "Loss after iteration 0: 1.113059\n",
      "Loss after iteration 0: 1.099919\n",
      "Loss after iteration 0: 1.066633\n",
      "Loss after iteration 0: 0.901343\n",
      "Loss after iteration 0: 1.082643\n",
      "Loss after iteration 0: 1.015204\n",
      "Loss after iteration 0: 1.075355\n",
      "Loss after iteration 0: 1.051888\n",
      "Loss after iteration 0: 1.027469\n",
      "Loss after iteration 0: 0.670938\n",
      "Loss after iteration 0: 1.145145\n",
      "Loss after iteration 0: 0.976483\n",
      "Loss after iteration 0: 0.978429\n",
      "Loss after iteration 0: 0.924537\n",
      "Loss after iteration 0: 1.088218\n",
      "Loss after iteration 0: 1.104770\n",
      "Loss after iteration 0: 1.016653\n",
      "Loss after iteration 0: 1.110696\n",
      "F1-Score on test set 0.5035971223021583\n"
     ]
    }
   ],
   "source": [
    "test_model = SimpleNN()\n",
    "test_model.add(activation=\"tanh\",input_dim=8, hidden_dim=16, output_dim=3, print_loss=True)\n",
    "test_model.train(uci_features_train, uci_labels_train, epoches=2000, reg_value=0.01, epsilon=0.01, batch_size=best_hp_value)\n",
    "prediction = test_model.infer(uci_features_test)\n",
    "confusion = confusion_matrix(uci_labels_test, prediction)\n",
    "confusion = np.array(confusion)\n",
    "test_f1_score = F1_score(confusion)\n",
    "print(\"F1-Score on test set\", test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-visitor",
   "metadata": {},
   "source": [
    "# Step V: Use a Neural Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-affiliation",
   "metadata": {},
   "source": [
    "- Let a neural network learn on the training set and report its evaluation metric on the **validation** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9508b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 3.637815\n",
      "Loss after iteration 0: 1.910060\n",
      "Loss after iteration 0: 4.183887\n",
      "Loss after iteration 0: 3.309771\n",
      "Loss after iteration 0: 3.656066\n",
      "Loss after iteration 0: 2.397539\n",
      "Loss after iteration 0: 2.634979\n",
      "Loss after iteration 0: 2.065391\n",
      "Loss after iteration 0: 1.488796\n",
      "Loss after iteration 0: 0.912948\n",
      "Loss after iteration 0: 1.038738\n",
      "Loss after iteration 0: 0.770882\n",
      "Loss after iteration 0: 1.020170\n",
      "Loss after iteration 0: 1.020745\n",
      "Loss after iteration 0: 1.097246\n",
      "Loss after iteration 0: 1.013267\n",
      "Loss after iteration 0: 1.092186\n",
      "Loss after iteration 0: 0.635765\n",
      "Loss after iteration 0: 1.031634\n",
      "Loss after iteration 0: 1.041832\n",
      "Loss after iteration 0: 1.080751\n",
      "Loss after iteration 0: 1.079451\n",
      "Loss after iteration 0: 1.079147\n",
      "Loss after iteration 0: 0.998578\n",
      "Loss after iteration 0: 0.775309\n",
      "Loss after iteration 0: 0.959161\n",
      "Loss after iteration 0: 1.088039\n",
      "Loss after iteration 0: 1.055747\n",
      "Loss after iteration 0: 0.975239\n",
      "Loss after iteration 0: 1.105002\n",
      "Loss after iteration 0: 1.037454\n",
      "Loss after iteration 0: 0.938885\n",
      "Loss after iteration 0: 0.848282\n",
      "Loss after iteration 0: 1.113089\n",
      "Loss after iteration 0: 0.971720\n",
      "Loss after iteration 0: 1.128043\n",
      "Loss after iteration 0: 1.016466\n",
      "Loss after iteration 0: 1.056370\n",
      "Loss after iteration 0: 0.905724\n",
      "Loss after iteration 0: 1.131764\n",
      "Loss after iteration 0: 1.000491\n",
      "Loss after iteration 0: 1.063340\n",
      "Loss after iteration 0: 1.042275\n",
      "Loss after iteration 0: 1.092281\n",
      "Loss after iteration 0: 0.991752\n",
      "Loss after iteration 0: 0.886505\n",
      "Loss after iteration 0: 0.988917\n",
      "Loss after iteration 0: 1.091930\n",
      "Loss after iteration 0: 1.038784\n",
      "Loss after iteration 0: 1.012391\n",
      "Loss after iteration 0: 0.976639\n",
      "Loss after iteration 0: 0.886764\n",
      "Loss after iteration 0: 0.974346\n",
      "Loss after iteration 0: 1.107530\n",
      "Loss after iteration 0: 1.087314\n",
      "Loss after iteration 0: 1.096992\n",
      "Loss after iteration 0: 1.084334\n",
      "Loss after iteration 0: 0.921470\n",
      "Loss after iteration 0: 1.089426\n",
      "Loss after iteration 0: 1.009150\n",
      "Loss after iteration 0: 1.059966\n",
      "Loss after iteration 0: 1.009613\n",
      "Loss after iteration 0: 1.010876\n",
      "Loss after iteration 0: 1.002660\n",
      "Loss after iteration 0: 1.056959\n",
      "Loss after iteration 0: 1.089226\n",
      "Loss after iteration 0: 1.085551\n",
      "Loss after iteration 0: 1.096272\n",
      "Loss after iteration 0: 1.084060\n",
      "Loss after iteration 0: 1.056580\n",
      "Loss after iteration 0: 1.084397\n",
      "Loss after iteration 0: 0.889340\n",
      "Loss after iteration 0: 1.083665\n",
      "Loss after iteration 0: 0.939350\n",
      "Loss after iteration 0: 1.081646\n",
      "Loss after iteration 0: 1.049706\n",
      "Loss after iteration 0: 1.090814\n",
      "Loss after iteration 0: 1.082790\n",
      "Loss after iteration 0: 0.809695\n",
      "Loss after iteration 0: 1.092307\n",
      "Loss after iteration 0: 1.121658\n",
      "Loss after iteration 0: 1.024109\n",
      "Loss after iteration 0: 1.013875\n",
      "Loss after iteration 0: 0.958526\n",
      "Loss after iteration 0: 1.085741\n",
      "Loss after iteration 0: 0.670507\n",
      "Loss after iteration 0: 0.956496\n",
      "Loss after iteration 0: 1.168676\n",
      "Loss after iteration 0: 1.001360\n",
      "Loss after iteration 0: 0.982883\n",
      "Loss after iteration 0: 1.083069\n",
      "Loss after iteration 0: 1.028335\n",
      "Loss after iteration 0: 1.033841\n",
      "Loss after iteration 0: 1.008733\n",
      "Loss after iteration 0: 0.850494\n",
      "Loss after iteration 0: 0.962766\n",
      "Loss after iteration 0: 1.157098\n",
      "Loss after iteration 0: 1.041960\n",
      "Loss after iteration 0: 1.005261\n",
      "Loss after iteration 0: 0.975313\n",
      "Loss after iteration 0: 1.112799\n",
      "Loss after iteration 0: 1.050585\n",
      "Loss after iteration 0: 0.875970\n",
      "Loss after iteration 0: 1.119337\n",
      "Loss after iteration 0: 0.989786\n",
      "Loss after iteration 0: 1.028001\n",
      "Loss after iteration 0: 0.986099\n",
      "Loss after iteration 0: 1.092750\n",
      "Loss after iteration 0: 1.121502\n",
      "Loss after iteration 0: 1.047963\n",
      "Loss after iteration 0: 1.025376\n",
      "Loss after iteration 0: 0.819121\n",
      "Loss after iteration 0: 1.115867\n",
      "Loss after iteration 0: 1.091381\n",
      "Loss after iteration 0: 1.092416\n",
      "Loss after iteration 0: 1.086361\n",
      "Loss after iteration 0: 1.042376\n",
      "Loss after iteration 0: 0.990090\n",
      "Loss after iteration 0: 1.080988\n",
      "Loss after iteration 0: 1.049343\n",
      "Loss after iteration 0: 1.084397\n",
      "Loss after iteration 0: 1.088501\n",
      "Loss after iteration 0: 0.994984\n",
      "Loss after iteration 0: 1.116230\n",
      "Loss after iteration 0: 1.080865\n",
      "Loss after iteration 0: 0.989525\n",
      "Loss after iteration 0: 1.069184\n",
      "Loss after iteration 0: 0.989748\n",
      "Loss after iteration 0: 1.047524\n",
      "Loss after iteration 0: 1.105115\n",
      "Loss after iteration 0: 1.084913\n",
      "Loss after iteration 0: 0.991262\n",
      "Loss after iteration 0: 1.027517\n",
      "Loss after iteration 0: 0.983473\n",
      "Loss after iteration 0: 1.048106\n",
      "Loss after iteration 0: 1.080058\n",
      "Loss after iteration 0: 1.075448\n",
      "Loss after iteration 0: 1.017770\n",
      "Loss after iteration 0: 1.102983\n",
      "Loss after iteration 0: 0.943769\n",
      "Loss after iteration 0: 1.091063\n",
      "Loss after iteration 0: 1.109020\n",
      "Loss after iteration 0: 0.935760\n",
      "Loss after iteration 0: 1.074400\n",
      "Loss after iteration 0: 0.997514\n",
      "Loss after iteration 0: 1.054860\n",
      "Loss after iteration 0: 0.877527\n",
      "Loss after iteration 0: 1.096415\n",
      "Loss after iteration 0: 0.835477\n",
      "Loss after iteration 0: 1.094106\n",
      "Loss after iteration 0: 0.883526\n",
      "Loss after iteration 0: 1.102900\n",
      "Loss after iteration 0: 0.843659\n",
      "Loss after iteration 0: 1.053998\n",
      "Loss after iteration 0: 1.085747\n",
      "Loss after iteration 0: 0.984026\n",
      "Loss after iteration 0: 0.870911\n",
      "Loss after iteration 0: 0.653717\n",
      "Loss after iteration 0: 1.181386\n",
      "Loss after iteration 0: 1.033714\n",
      "Loss after iteration 0: 0.866818\n",
      "Loss after iteration 0: 0.968098\n",
      "Loss after iteration 0: 0.823355\n",
      "Loss after iteration 0: 1.135356\n",
      "Loss after iteration 0: 1.113833\n",
      "Loss after iteration 0: 1.099431\n",
      "Loss after iteration 0: 1.047716\n",
      "Loss after iteration 0: 0.973159\n",
      "Loss after iteration 0: 0.963097\n",
      "Loss after iteration 0: 0.818774\n",
      "Loss after iteration 0: 1.119882\n",
      "Loss after iteration 0: 1.108921\n",
      "Loss after iteration 0: 1.031809\n",
      "Loss after iteration 0: 1.048230\n",
      "Loss after iteration 0: 0.924056\n",
      "Loss after iteration 0: 0.874786\n",
      "Loss after iteration 0: 1.147640\n",
      "Loss after iteration 0: 0.924435\n",
      "Loss after iteration 0: 1.014536\n",
      "Loss after iteration 0: 0.660331\n",
      "Loss after iteration 0: 1.137554\n",
      "Loss after iteration 0: 1.100567\n",
      "Loss after iteration 0: 1.006195\n",
      "Loss after iteration 0: 1.107872\n",
      "Loss after iteration 0: 1.042530\n",
      "Loss after iteration 0: 1.087081\n",
      "Loss after iteration 0: 1.014603\n",
      "Loss after iteration 0: 0.899864\n",
      "Loss after iteration 0: 1.019397\n",
      "Loss after iteration 0: 1.086797\n",
      "Loss after iteration 0: 0.656472\n",
      "Loss after iteration 0: 1.097815\n",
      "Loss after iteration 0: 1.081254\n",
      "Loss after iteration 0: 1.097027\n",
      "Loss after iteration 0: 1.023791\n",
      "Loss after iteration 0: 1.072768\n",
      "Loss after iteration 0: 1.046917\n",
      "Loss after iteration 0: 1.070078\n",
      "Loss after iteration 0: 1.089263\n",
      "Loss after iteration 0: 0.930006\n",
      "Loss after iteration 0: 1.068031\n",
      "Loss after iteration 0: 1.015338\n",
      "Loss after iteration 0: 1.045205\n",
      "Loss after iteration 0: 1.091674\n",
      "Loss after iteration 0: 1.028737\n",
      "Loss after iteration 0: 1.029489\n",
      "Loss after iteration 0: 1.087413\n",
      "Loss after iteration 0: 1.086585\n",
      "Loss after iteration 0: 1.065298\n",
      "Loss after iteration 0: 1.017040\n",
      "Loss after iteration 0: 0.981800\n",
      "Loss after iteration 0: 0.905880\n",
      "Loss after iteration 0: 1.091238\n",
      "Loss after iteration 0: 1.108008\n",
      "Loss after iteration 0: 0.941504\n",
      "Loss after iteration 0: 1.061963\n",
      "Loss after iteration 0: 0.992621\n",
      "Loss after iteration 0: 1.083594\n",
      "Loss after iteration 0: 0.989865\n",
      "Loss after iteration 0: 1.040208\n",
      "Loss after iteration 0: 0.605506\n",
      "Loss after iteration 0: 1.122864\n",
      "Loss after iteration 0: 0.974127\n",
      "Loss after iteration 0: 1.006838\n",
      "Loss after iteration 0: 1.115766\n",
      "Loss after iteration 0: 1.090921\n",
      "Loss after iteration 0: 1.105733\n",
      "Loss after iteration 0: 1.083615\n",
      "Loss after iteration 0: 1.091675\n",
      "Loss after iteration 0: 1.083277\n",
      "Loss after iteration 0: 0.843311\n",
      "Loss after iteration 0: 1.014582\n",
      "Loss after iteration 0: 1.117453\n",
      "Loss after iteration 0: 1.039176\n",
      "Loss after iteration 0: 0.876884\n",
      "Loss after iteration 0: 0.990640\n",
      "Loss after iteration 0: 0.822444\n",
      "Loss after iteration 0: 1.148606\n",
      "Loss after iteration 0: 0.997796\n",
      "Loss after iteration 0: 1.079661\n",
      "Loss after iteration 0: 0.875096\n",
      "Loss after iteration 0: 0.891520\n",
      "Loss after iteration 0: 1.085788\n",
      "Loss after iteration 0: 1.068111\n",
      "Loss after iteration 0: 0.958135\n",
      "Loss after iteration 0: 1.018449\n",
      "Loss after iteration 0: 1.011852\n",
      "Loss after iteration 0: 0.764531\n",
      "Loss after iteration 0: 1.073631\n",
      "Loss after iteration 0: 1.083582\n",
      "Loss after iteration 0: 0.938198\n",
      "Loss after iteration 0: 1.120344\n",
      "Loss after iteration 0: 1.035976\n",
      "Loss after iteration 0: 1.083800\n",
      "Loss after iteration 0: 1.039436\n",
      "Loss after iteration 0: 0.990881\n",
      "Loss after iteration 0: 1.084249\n",
      "Loss after iteration 0: 1.081317\n",
      "Loss after iteration 0: 0.996884\n",
      "Loss after iteration 0: 1.003630\n",
      "Loss after iteration 0: 0.976128\n",
      "Loss after iteration 0: 0.977394\n",
      "Loss after iteration 0: 0.919819\n",
      "Loss after iteration 0: 0.868742\n",
      "Loss after iteration 0: 1.105481\n",
      "Loss after iteration 0: 1.081420\n",
      "Loss after iteration 0: 1.044765\n",
      "Loss after iteration 0: 0.971813\n",
      "Loss after iteration 0: 1.116933\n",
      "Loss after iteration 0: 1.010725\n",
      "Loss after iteration 0: 1.021098\n",
      "Loss after iteration 0: 0.967172\n",
      "Loss after iteration 0: 1.044034\n",
      "Loss after iteration 0: 0.693617\n",
      "Loss after iteration 0: 1.051396\n",
      "Loss after iteration 0: 1.064386\n",
      "Loss after iteration 0: 1.032484\n",
      "Loss after iteration 0: 0.842132\n",
      "Loss after iteration 0: 1.088240\n",
      "Loss after iteration 0: 1.037313\n",
      "Loss after iteration 0: 1.122995\n",
      "Loss after iteration 0: 1.045012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.872116\n",
      "Loss after iteration 0: 0.887714\n",
      "Loss after iteration 0: 1.098787\n",
      "Loss after iteration 0: 1.080912\n",
      "Loss after iteration 0: 0.860814\n",
      "Loss after iteration 0: 1.090966\n",
      "Loss after iteration 0: 1.069097\n",
      "Loss after iteration 0: 1.033859\n",
      "Loss after iteration 0: 0.987290\n",
      "Loss after iteration 0: 0.992071\n",
      "Loss after iteration 0: 1.080128\n",
      "Loss after iteration 0: 1.030221\n",
      "Loss after iteration 0: 1.083283\n",
      "Loss after iteration 0: 0.969092\n",
      "Loss after iteration 0: 1.094777\n",
      "Loss after iteration 0: 1.025510\n",
      "Loss after iteration 0: 0.863770\n",
      "Loss after iteration 0: 1.143908\n",
      "Loss after iteration 0: 1.028453\n",
      "Loss after iteration 0: 1.108014\n",
      "Loss after iteration 0: 0.890174\n",
      "Loss after iteration 0: 1.116352\n",
      "Loss after iteration 0: 1.006038\n",
      "Loss after iteration 0: 1.083451\n",
      "Loss after iteration 0: 0.991351\n",
      "Loss after iteration 0: 0.972699\n",
      "Loss after iteration 0: 1.032585\n",
      "Loss after iteration 0: 1.078601\n",
      "Loss after iteration 0: 1.079687\n",
      "Loss after iteration 0: 0.992328\n",
      "Loss after iteration 0: 1.102283\n",
      "Loss after iteration 0: 1.082839\n",
      "Loss after iteration 0: 1.012309\n",
      "Loss after iteration 0: 0.882562\n",
      "Loss after iteration 0: 1.127258\n",
      "Loss after iteration 0: 1.081024\n",
      "Loss after iteration 0: 1.079658\n",
      "Loss after iteration 0: 1.040431\n",
      "Loss after iteration 0: 0.657624\n",
      "Loss after iteration 0: 0.939138\n",
      "Loss after iteration 0: 1.100933\n",
      "Loss after iteration 0: 1.015777\n",
      "Loss after iteration 0: 1.094837\n",
      "Loss after iteration 0: 1.086367\n",
      "Loss after iteration 0: 1.092107\n",
      "Loss after iteration 0: 0.992054\n",
      "Loss after iteration 0: 1.081026\n",
      "Loss after iteration 0: 1.030827\n",
      "Loss after iteration 0: 1.084956\n",
      "Loss after iteration 0: 1.111863\n",
      "Loss after iteration 0: 1.018844\n",
      "Loss after iteration 0: 1.042198\n",
      "Loss after iteration 0: 0.853925\n",
      "Loss after iteration 0: 1.021043\n",
      "Loss after iteration 0: 1.029037\n",
      "Loss after iteration 0: 1.018415\n",
      "Loss after iteration 0: 1.042540\n",
      "Loss after iteration 0: 0.993208\n",
      "Loss after iteration 0: 1.070476\n",
      "Loss after iteration 0: 0.997039\n",
      "Loss after iteration 0: 0.971139\n",
      "Loss after iteration 0: 1.082665\n",
      "Loss after iteration 0: 1.053465\n",
      "Loss after iteration 0: 0.921017\n",
      "Loss after iteration 0: 0.964095\n",
      "Loss after iteration 0: 1.091843\n",
      "Loss after iteration 0: 1.063995\n",
      "Loss after iteration 0: 1.080194\n",
      "Loss after iteration 0: 1.044155\n",
      "Loss after iteration 0: 1.111992\n",
      "Loss after iteration 0: 1.090203\n",
      "Loss after iteration 0: 1.019369\n",
      "Loss after iteration 0: 0.855917\n",
      "Loss after iteration 0: 1.056241\n",
      "Loss after iteration 0: 0.616897\n",
      "Loss after iteration 0: 1.112038\n",
      "Loss after iteration 0: 1.136847\n",
      "Loss after iteration 0: 1.090313\n",
      "Loss after iteration 0: 1.001717\n",
      "Loss after iteration 0: 0.953718\n",
      "Loss after iteration 0: 1.025365\n",
      "Loss after iteration 0: 0.987005\n",
      "Loss after iteration 0: 1.074009\n",
      "Loss after iteration 0: 1.027688\n",
      "Loss after iteration 0: 1.086679\n",
      "Loss after iteration 0: 0.934258\n",
      "Loss after iteration 0: 1.080560\n",
      "Loss after iteration 0: 1.048248\n",
      "Loss after iteration 0: 0.691038\n",
      "Loss after iteration 0: 0.957090\n",
      "Loss after iteration 0: 1.010269\n",
      "Loss after iteration 0: 1.162465\n",
      "Loss after iteration 0: 0.977590\n",
      "Loss after iteration 0: 1.112193\n",
      "Loss after iteration 0: 0.834923\n",
      "Loss after iteration 0: 1.003784\n",
      "Loss after iteration 0: 0.864369\n",
      "Loss after iteration 0: 1.095708\n",
      "Loss after iteration 0: 1.117122\n",
      "Loss after iteration 0: 1.079859\n",
      "Loss after iteration 0: 0.892600\n",
      "Loss after iteration 0: 1.106345\n",
      "Loss after iteration 0: 1.013554\n",
      "Loss after iteration 0: 1.052711\n",
      "Loss after iteration 0: 1.091149\n",
      "Loss after iteration 0: 1.060020\n",
      "Loss after iteration 0: 1.040951\n",
      "Loss after iteration 0: 0.991500\n",
      "Loss after iteration 0: 1.051928\n",
      "Loss after iteration 0: 1.074472\n",
      "Loss after iteration 0: 0.726108\n",
      "Loss after iteration 0: 1.072421\n",
      "Loss after iteration 0: 1.082332\n",
      "Loss after iteration 0: 1.093750\n",
      "Loss after iteration 0: 1.084239\n",
      "Loss after iteration 0: 0.937884\n",
      "Loss after iteration 0: 1.032272\n",
      "Loss after iteration 0: 0.872759\n",
      "Loss after iteration 0: 1.113059\n",
      "Loss after iteration 0: 1.099919\n",
      "Loss after iteration 0: 1.066633\n",
      "Loss after iteration 0: 0.901343\n",
      "Loss after iteration 0: 1.082643\n",
      "Loss after iteration 0: 1.015204\n",
      "Loss after iteration 0: 1.075355\n",
      "Loss after iteration 0: 1.051888\n",
      "Loss after iteration 0: 1.027469\n",
      "Loss after iteration 0: 0.670938\n",
      "Loss after iteration 0: 1.145145\n",
      "Loss after iteration 0: 0.976483\n",
      "Loss after iteration 0: 0.978429\n",
      "Loss after iteration 0: 0.924537\n",
      "Loss after iteration 0: 1.088218\n",
      "Loss after iteration 0: 1.104770\n",
      "Loss after iteration 0: 1.016653\n",
      "Loss after iteration 0: 1.110696\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN()\n",
    "model.add(activation=\"tanh\",input_dim=8, hidden_dim=16, output_dim=3, print_loss=True)\n",
    "model.train(uci_features_train, uci_labels_train, epoches=1000, reg_value=0.01, epsilon=0.01, batch_size=best_hp_value)\n",
    "\n",
    "prediction = test_model.infer(uci_features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fbfe641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(uci_labels_valid, prediction)\n",
    "confusion = np.array(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "42d1e076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision\n",
    "p = Precision(confusion)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4aa65efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall\n",
    "r = Recall(confusion)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b63b0d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f1-score\n",
    "f1_score = F1_score(confusion)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "33454af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.36 %\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "from sklearn import metrics\n",
    "print('Accuracy: %2.2f %%' % (100. * metrics.accuracy_score(uci_labels_test, pred_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-marina",
   "metadata": {},
   "source": [
    "# Step VI: Experiment (2) Hyperparameter Choice of Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-clothing",
   "metadata": {},
   "source": [
    "- choose a hyperparameter of your neural net, e.g. the number of neurons in the first hidden layer or the learning rate for SGD\n",
    "- (iteratively) create models for each hyperparameter setting, e.g. the number of neurons h=10,20,30,40,50,60,70,80,90,100\n",
    "- train the neural net on the train set and evaluate it over your validation set\n",
    "- keep all models with each hyperparameter setting and determine which is the best performing model on the validation set\n",
    "- evaluate them also on the test set. is the best model on the validation set with its hyperparameter also the best model on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "novel-consumer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 3.827339\n",
      "Loss after iteration 0: 2.143076\n",
      "Loss after iteration 0: 4.152100\n",
      "Loss after iteration 0: 3.035552\n",
      "Loss after iteration 0: 2.576459\n",
      "Loss after iteration 0: 3.913574\n",
      "Loss after iteration 0: 2.613579\n",
      "Loss after iteration 0: 2.570164\n",
      "Loss after iteration 0: 2.513259\n",
      "Loss after iteration 0: 1.288972\n",
      "Loss after iteration 0: 3.639638\n",
      "Loss after iteration 0: 4.169488\n",
      "Loss after iteration 0: 2.948210\n",
      "Loss after iteration 0: 3.403006\n",
      "Loss after iteration 0: 2.252927\n",
      "Loss after iteration 0: 2.138461\n",
      "Loss after iteration 0: 2.164974\n",
      "Loss after iteration 0: 3.839160\n",
      "Loss after iteration 0: 2.156264\n",
      "Loss after iteration 0: 3.072942\n",
      "Loss after iteration 0: 2.547854\n",
      "Loss after iteration 0: 2.451016\n",
      "Loss after iteration 0: 2.422580\n",
      "Loss after iteration 0: 3.041085\n",
      "Loss after iteration 0: 3.149859\n",
      "Loss after iteration 0: 0.864811\n",
      "Loss after iteration 0: 1.910311\n",
      "Loss after iteration 0: 2.002151\n",
      "Loss after iteration 0: 1.480077\n",
      "Loss after iteration 0: 2.254736\n",
      "Loss after iteration 0: 2.080751\n",
      "Loss after iteration 0: 3.061875\n",
      "Loss after iteration 0: 2.590777\n",
      "Loss after iteration 0: 2.077384\n",
      "Loss after iteration 0: 1.998168\n",
      "Loss after iteration 0: 2.771948\n",
      "Loss after iteration 0: 3.120712\n",
      "Loss after iteration 0: 2.467521\n",
      "Loss after iteration 0: 2.466650\n",
      "Loss after iteration 0: 1.660761\n",
      "Loss after iteration 0: 1.426914\n",
      "Loss after iteration 0: 1.150061\n",
      "Loss after iteration 0: 2.529277\n",
      "Loss after iteration 0: 1.525734\n",
      "Loss after iteration 0: 1.372169\n",
      "Loss after iteration 0: 0.870971\n",
      "Loss after iteration 0: 2.210198\n",
      "Loss after iteration 0: 1.709154\n",
      "Loss after iteration 0: 1.277206\n",
      "Loss after iteration 0: 1.961672\n",
      "Loss after iteration 0: 1.825988\n",
      "Loss after iteration 0: 1.492522\n",
      "Loss after iteration 0: 1.823400\n",
      "Loss after iteration 0: 1.324475\n",
      "Loss after iteration 0: 1.043598\n",
      "Loss after iteration 0: 1.252086\n",
      "Loss after iteration 0: 1.241385\n",
      "Loss after iteration 0: 1.580320\n",
      "Loss after iteration 0: 1.046064\n",
      "Loss after iteration 0: 1.615545\n",
      "Loss after iteration 0: 1.105345\n",
      "Loss after iteration 0: 1.130551\n",
      "Loss after iteration 0: 1.561372\n",
      "Loss after iteration 0: 1.608396\n",
      "Loss after iteration 0: 0.960929\n",
      "Loss after iteration 0: 1.377180\n",
      "Loss after iteration 0: 1.166323\n",
      "Loss after iteration 0: 1.232944\n",
      "Loss after iteration 0: 1.229017\n",
      "Loss after iteration 0: 1.273008\n",
      "Loss after iteration 0: 1.334681\n",
      "Loss after iteration 0: 0.885841\n",
      "Loss after iteration 0: 1.145693\n",
      "Loss after iteration 0: 1.159558\n",
      "Loss after iteration 0: 1.205669\n",
      "Loss after iteration 0: 1.415262\n",
      "Loss after iteration 0: 1.133525\n",
      "Loss after iteration 0: 1.132193\n",
      "Loss after iteration 0: 0.984872\n",
      "Loss after iteration 0: 1.125971\n",
      "Loss after iteration 0: 1.191123\n",
      "Loss after iteration 0: 1.248484\n",
      "Loss after iteration 0: 1.416054\n",
      "Loss after iteration 0: 1.125912\n",
      "Loss after iteration 0: 1.252693\n",
      "Loss after iteration 0: 0.818219\n",
      "Loss after iteration 0: 1.033114\n",
      "Loss after iteration 0: 1.182729\n",
      "Loss after iteration 0: 1.302379\n",
      "Loss after iteration 0: 1.364315\n",
      "Loss after iteration 0: 1.224982\n",
      "Loss after iteration 0: 1.096846\n",
      "Loss after iteration 0: 0.976017\n",
      "Loss after iteration 0: 1.209360\n",
      "Loss after iteration 0: 0.851088\n",
      "Loss after iteration 0: 0.976807\n",
      "Loss after iteration 0: 1.223981\n",
      "Loss after iteration 0: 1.272305\n",
      "Loss after iteration 0: 1.315524\n",
      "Loss after iteration 0: 1.240934\n",
      "Loss after iteration 0: 1.091058\n",
      "Loss after iteration 0: 1.094002\n",
      "Loss after iteration 0: 1.261256\n",
      "Loss after iteration 0: 1.140236\n",
      "Loss after iteration 0: 1.263006\n",
      "Loss after iteration 0: 1.212888\n",
      "Loss after iteration 0: 1.170599\n",
      "Loss after iteration 0: 1.124599\n",
      "Loss after iteration 0: 1.085169\n",
      "Loss after iteration 0: 1.043280\n",
      "Loss after iteration 0: 1.042535\n",
      "Loss after iteration 0: 1.330982\n",
      "Loss after iteration 0: 1.084507\n",
      "Loss after iteration 0: 1.083888\n",
      "Loss after iteration 0: 1.120022\n",
      "Loss after iteration 0: 1.086278\n",
      "Loss after iteration 0: 1.110886\n",
      "Loss after iteration 0: 1.167998\n",
      "Loss after iteration 0: 1.091165\n",
      "Loss after iteration 0: 1.105499\n",
      "Loss after iteration 0: 1.142044\n",
      "Loss after iteration 0: 1.081510\n",
      "Loss after iteration 0: 1.151011\n",
      "Loss after iteration 0: 1.118688\n",
      "Loss after iteration 0: 1.131640\n",
      "Loss after iteration 0: 1.173037\n",
      "Loss after iteration 0: 1.033615\n",
      "Loss after iteration 0: 1.041507\n",
      "Loss after iteration 0: 1.101051\n",
      "Loss after iteration 0: 1.129335\n",
      "Loss after iteration 0: 1.114364\n",
      "Loss after iteration 0: 1.093174\n",
      "Loss after iteration 0: 1.061015\n",
      "Loss after iteration 0: 1.069951\n",
      "Loss after iteration 0: 1.172677\n",
      "Loss after iteration 0: 1.124079\n",
      "Loss after iteration 0: 1.068476\n",
      "Loss after iteration 0: 1.131808\n",
      "Loss after iteration 0: 1.112798\n",
      "Loss after iteration 0: 1.024810\n",
      "Loss after iteration 0: 1.112513\n",
      "Loss after iteration 0: 1.124399\n",
      "Loss after iteration 0: 1.097624\n",
      "Loss after iteration 0: 1.098798\n",
      "Loss after iteration 0: 1.090876\n",
      "Loss after iteration 0: 1.141237\n",
      "Loss after iteration 0: 1.147162\n",
      "Loss after iteration 0: 1.100711\n",
      "Loss after iteration 0: 1.076462\n",
      "Loss after iteration 0: 1.056093\n",
      "Loss after iteration 0: 1.020524\n",
      "Loss after iteration 0: 1.097397\n",
      "Loss after iteration 0: 1.091565\n",
      "Loss after iteration 0: 1.123666\n",
      "Loss after iteration 0: 1.091389\n",
      "Loss after iteration 0: 1.048604\n",
      "Loss after iteration 0: 1.020744\n",
      "Loss after iteration 0: 1.040691\n",
      "Loss after iteration 0: 1.132409\n",
      "Loss after iteration 0: 1.052800\n",
      "Loss after iteration 0: 1.003999\n",
      "Loss after iteration 0: 1.042609\n",
      "Loss after iteration 0: 0.992140\n",
      "Loss after iteration 0: 1.094246\n",
      "Loss after iteration 0: 1.096229\n",
      "Loss after iteration 0: 1.142812\n",
      "Loss after iteration 0: 1.138112\n",
      "Loss after iteration 0: 1.084267\n",
      "Loss after iteration 0: 1.076910\n",
      "Loss after iteration 0: 1.203487\n",
      "Loss after iteration 0: 1.089088\n",
      "Loss after iteration 0: 1.141934\n",
      "Loss after iteration 0: 1.042763\n",
      "Loss after iteration 0: 1.054126\n",
      "Loss after iteration 0: 1.098293\n",
      "Loss after iteration 0: 1.135702\n",
      "Loss after iteration 0: 1.077029\n",
      "Loss after iteration 0: 1.081005\n",
      "Loss after iteration 0: 1.039424\n",
      "Loss after iteration 0: 0.975843\n",
      "Loss after iteration 0: 1.184655\n",
      "Loss after iteration 0: 1.105494\n",
      "Loss after iteration 0: 1.071987\n",
      "Loss after iteration 0: 1.132350\n",
      "Loss after iteration 0: 1.055967\n",
      "Loss after iteration 0: 1.086851\n",
      "Loss after iteration 0: 1.110242\n",
      "Loss after iteration 0: 1.137447\n",
      "Loss after iteration 0: 1.035041\n",
      "Loss after iteration 0: 1.099909\n",
      "Loss after iteration 0: 1.017068\n",
      "Loss after iteration 0: 1.086559\n",
      "Loss after iteration 0: 1.125664\n",
      "Loss after iteration 0: 1.132595\n",
      "Loss after iteration 0: 1.060705\n",
      "Loss after iteration 0: 1.122716\n",
      "Loss after iteration 0: 1.055216\n",
      "Loss after iteration 0: 1.133408\n",
      "Loss after iteration 0: 1.126657\n",
      "Loss after iteration 0: 1.139489\n",
      "Loss after iteration 0: 1.097441\n",
      "Loss after iteration 0: 1.062349\n",
      "Loss after iteration 0: 1.147170\n",
      "Loss after iteration 0: 1.093614\n",
      "Loss after iteration 0: 1.089077\n",
      "Loss after iteration 0: 1.065011\n",
      "Loss after iteration 0: 1.093321\n",
      "Loss after iteration 0: 1.093927\n",
      "Loss after iteration 0: 1.117569\n",
      "Loss after iteration 0: 1.139244\n",
      "Loss after iteration 0: 1.129978\n",
      "Loss after iteration 0: 1.116569\n",
      "Loss after iteration 0: 1.108668\n",
      "Loss after iteration 0: 1.037433\n",
      "Loss after iteration 0: 1.117461\n",
      "Loss after iteration 0: 1.090717\n",
      "Loss after iteration 0: 1.084148\n",
      "Loss after iteration 0: 1.091432\n",
      "Loss after iteration 0: 1.070582\n",
      "Loss after iteration 0: 1.074922\n",
      "Loss after iteration 0: 1.011186\n",
      "Loss after iteration 0: 1.114056\n",
      "Loss after iteration 0: 1.079469\n",
      "Loss after iteration 0: 1.101114\n",
      "Loss after iteration 0: 1.091693\n",
      "Loss after iteration 0: 1.090483\n",
      "Loss after iteration 0: 1.117329\n",
      "Loss after iteration 0: 1.090828\n",
      "Loss after iteration 0: 1.078989\n",
      "Loss after iteration 0: 1.113944\n",
      "Loss after iteration 0: 1.056387\n",
      "Loss after iteration 0: 1.092206\n",
      "Loss after iteration 0: 1.114487\n",
      "Loss after iteration 0: 1.095040\n",
      "Loss after iteration 0: 1.067323\n",
      "Loss after iteration 0: 1.050814\n",
      "Loss after iteration 0: 1.023733\n",
      "Loss after iteration 0: 1.048948\n",
      "Loss after iteration 0: 1.153259\n",
      "Loss after iteration 0: 1.103602\n",
      "Loss after iteration 0: 1.016197\n",
      "Loss after iteration 0: 1.127210\n",
      "Loss after iteration 0: 1.102584\n",
      "Loss after iteration 0: 1.128573\n",
      "Loss after iteration 0: 1.038343\n",
      "Loss after iteration 0: 1.066996\n",
      "Loss after iteration 0: 1.065295\n",
      "Loss after iteration 0: 1.004778\n",
      "Loss after iteration 0: 1.122815\n",
      "Loss after iteration 0: 1.085264\n",
      "Loss after iteration 0: 1.163115\n",
      "Loss after iteration 0: 1.105494\n",
      "Loss after iteration 0: 1.093781\n",
      "Loss after iteration 0: 1.114070\n",
      "Loss after iteration 0: 1.120000\n",
      "Loss after iteration 0: 1.136057\n",
      "Loss after iteration 0: 1.109718\n",
      "Loss after iteration 0: 1.108140\n",
      "Loss after iteration 0: 0.997665\n",
      "Loss after iteration 0: 1.067152\n",
      "Loss after iteration 0: 1.055234\n",
      "Loss after iteration 0: 1.155347\n",
      "Loss after iteration 0: 1.034217\n",
      "Loss after iteration 0: 1.053973\n",
      "Loss after iteration 0: 1.077310\n",
      "Loss after iteration 0: 1.085160\n",
      "Loss after iteration 0: 1.078513\n",
      "Loss after iteration 0: 1.157316\n",
      "Loss after iteration 0: 1.137684\n",
      "Loss after iteration 0: 1.050015\n",
      "Loss after iteration 0: 1.111112\n",
      "Loss after iteration 0: 1.049319\n",
      "Loss after iteration 0: 1.140505\n",
      "Loss after iteration 0: 0.967383\n",
      "Loss after iteration 0: 1.146433\n",
      "Loss after iteration 0: 1.077685\n",
      "Loss after iteration 0: 1.078192\n",
      "Loss after iteration 0: 1.005203\n",
      "Loss after iteration 0: 1.081407\n",
      "Loss after iteration 0: 1.079361\n",
      "Loss after iteration 0: 1.116411\n",
      "Loss after iteration 0: 1.077532\n",
      "Loss after iteration 0: 1.172506\n",
      "Loss after iteration 0: 0.995644\n",
      "Loss after iteration 0: 1.112863\n",
      "Loss after iteration 0: 1.202780\n",
      "Loss after iteration 0: 0.992105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.080662\n",
      "Loss after iteration 0: 1.098161\n",
      "Loss after iteration 0: 1.087580\n",
      "Loss after iteration 0: 1.187594\n",
      "Loss after iteration 0: 1.058965\n",
      "Loss after iteration 0: 1.109252\n",
      "Loss after iteration 0: 1.154109\n",
      "Loss after iteration 0: 1.117945\n",
      "Loss after iteration 0: 1.056161\n",
      "Loss after iteration 0: 1.083112\n",
      "Loss after iteration 0: 1.067543\n",
      "Loss after iteration 0: 1.088496\n",
      "Loss after iteration 0: 1.120135\n",
      "Loss after iteration 0: 1.149498\n",
      "Loss after iteration 0: 1.087354\n",
      "Loss after iteration 0: 1.239439\n",
      "Loss after iteration 0: 1.088265\n",
      "Loss after iteration 0: 1.050666\n",
      "Loss after iteration 0: 1.085933\n",
      "Loss after iteration 0: 1.066294\n",
      "Loss after iteration 0: 1.061438\n",
      "Loss after iteration 0: 1.094390\n",
      "Loss after iteration 0: 1.055175\n",
      "Loss after iteration 0: 1.082522\n",
      "Loss after iteration 0: 1.158980\n",
      "Loss after iteration 0: 1.085897\n",
      "Loss after iteration 0: 1.101970\n",
      "Loss after iteration 0: 1.044338\n",
      "Loss after iteration 0: 1.032842\n",
      "Loss after iteration 0: 1.146422\n",
      "Loss after iteration 0: 1.085275\n",
      "Loss after iteration 0: 1.083219\n",
      "Loss after iteration 0: 1.099590\n",
      "Loss after iteration 0: 0.958697\n",
      "Loss after iteration 0: 1.036351\n",
      "Loss after iteration 0: 1.082654\n",
      "Loss after iteration 0: 1.059367\n",
      "Loss after iteration 0: 1.081174\n",
      "Loss after iteration 0: 1.081165\n",
      "Loss after iteration 0: 1.127740\n",
      "Loss after iteration 0: 1.075662\n",
      "Loss after iteration 0: 1.106510\n",
      "Loss after iteration 0: 1.048423\n",
      "Loss after iteration 0: 1.099261\n",
      "Loss after iteration 0: 1.079746\n",
      "Loss after iteration 0: 1.059898\n",
      "Loss after iteration 0: 1.119581\n",
      "Loss after iteration 0: 0.974267\n",
      "Loss after iteration 0: 1.194327\n",
      "Loss after iteration 0: 1.118937\n",
      "Loss after iteration 0: 1.141739\n",
      "Loss after iteration 0: 1.099587\n",
      "Loss after iteration 0: 1.146398\n",
      "Loss after iteration 0: 1.086057\n",
      "Loss after iteration 0: 1.139942\n",
      "Loss after iteration 0: 1.129884\n",
      "Loss after iteration 0: 1.070394\n",
      "Loss after iteration 0: 1.122417\n",
      "Loss after iteration 0: 1.086196\n",
      "Loss after iteration 0: 1.096903\n",
      "Loss after iteration 0: 1.101945\n",
      "Loss after iteration 0: 1.113757\n",
      "Loss after iteration 0: 1.099810\n",
      "Loss after iteration 0: 1.078993\n",
      "Loss after iteration 0: 1.108347\n",
      "Loss after iteration 0: 1.106571\n",
      "Loss after iteration 0: 1.078559\n",
      "Loss after iteration 0: 1.049230\n",
      "Loss after iteration 0: 1.079325\n",
      "Loss after iteration 0: 1.005839\n",
      "Loss after iteration 0: 1.085115\n",
      "Loss after iteration 0: 1.129688\n",
      "Loss after iteration 0: 1.104808\n",
      "Loss after iteration 0: 1.054694\n",
      "Loss after iteration 0: 1.089706\n",
      "Loss after iteration 0: 1.131225\n",
      "Loss after iteration 0: 1.099885\n",
      "Loss after iteration 0: 1.061998\n",
      "Loss after iteration 0: 1.078888\n",
      "Loss after iteration 0: 1.080824\n",
      "Loss after iteration 0: 1.084139\n",
      "Loss after iteration 0: 1.095219\n",
      "Loss after iteration 0: 1.084570\n",
      "Loss after iteration 0: 0.994986\n",
      "Loss after iteration 0: 1.051411\n",
      "Loss after iteration 0: 1.096585\n",
      "Loss after iteration 0: 1.088695\n",
      "Loss after iteration 0: 1.182701\n",
      "Loss after iteration 0: 1.112433\n",
      "Loss after iteration 0: 1.011023\n",
      "Loss after iteration 0: 1.105724\n",
      "Loss after iteration 0: 1.008902\n",
      "Loss after iteration 0: 1.076456\n",
      "Loss after iteration 0: 1.114307\n",
      "Loss after iteration 0: 1.081529\n",
      "Loss after iteration 0: 1.026618\n",
      "Loss after iteration 0: 1.112144\n",
      "Loss after iteration 0: 1.047483\n",
      "Loss after iteration 0: 1.139462\n",
      "Loss after iteration 0: 1.083340\n",
      "Loss after iteration 0: 1.091704\n",
      "Loss after iteration 0: 1.135637\n",
      "Loss after iteration 0: 1.089618\n",
      "Loss after iteration 0: 1.056515\n",
      "Loss after iteration 0: 1.144285\n",
      "Loss after iteration 0: 0.962289\n",
      "Loss after iteration 0: 1.135377\n",
      "Loss after iteration 0: 1.083027\n",
      "Loss after iteration 0: 1.122454\n",
      "Loss after iteration 0: 1.082048\n",
      "Loss after iteration 0: 1.117337\n",
      "Loss after iteration 0: 1.153808\n",
      "Loss after iteration 0: 1.160293\n",
      "Loss after iteration 0: 1.091366\n",
      "Loss after iteration 0: 1.105364\n",
      "Loss after iteration 0: 1.075847\n",
      "Loss after iteration 0: 1.026770\n",
      "Loss after iteration 0: 1.083638\n",
      "Loss after iteration 0: 1.091277\n",
      "Loss after iteration 0: 1.125930\n",
      "Loss after iteration 0: 1.121067\n",
      "Loss after iteration 0: 1.127498\n",
      "Loss after iteration 0: 1.115723\n",
      "Loss after iteration 0: 1.104879\n",
      "Loss after iteration 0: 1.045694\n",
      "Loss after iteration 0: 1.060267\n",
      "Loss after iteration 0: 1.052460\n",
      "Loss after iteration 0: 1.083834\n",
      "Loss after iteration 0: 1.131995\n",
      "Loss after iteration 0: 1.127464\n",
      "Loss after iteration 0: 1.099698\n",
      "Loss after iteration 0: 3.935769\n",
      "Loss after iteration 0: 2.535228\n",
      "Loss after iteration 0: 5.517799\n",
      "Loss after iteration 0: 1.673281\n",
      "Loss after iteration 0: 2.803393\n",
      "Loss after iteration 0: 3.530900\n",
      "Loss after iteration 0: 2.039076\n",
      "Loss after iteration 0: 1.793897\n",
      "Loss after iteration 0: 1.612835\n",
      "Loss after iteration 0: 1.437955\n",
      "Loss after iteration 0: 1.365508\n",
      "Loss after iteration 0: 1.350425\n",
      "Loss after iteration 0: 1.363017\n",
      "Loss after iteration 0: 1.113831\n",
      "Loss after iteration 0: 1.154870\n",
      "Loss after iteration 0: 1.027337\n",
      "Loss after iteration 0: 1.138105\n",
      "Loss after iteration 0: 1.334481\n",
      "Loss after iteration 0: 1.164319\n",
      "Loss after iteration 0: 0.935724\n",
      "Loss after iteration 0: 1.019552\n",
      "Loss after iteration 0: 1.039414\n",
      "Loss after iteration 0: 1.081837\n",
      "Loss after iteration 0: 1.150320\n",
      "Loss after iteration 0: 0.800742\n",
      "Loss after iteration 0: 0.972714\n",
      "Loss after iteration 0: 1.024984\n",
      "Loss after iteration 0: 1.167791\n",
      "Loss after iteration 0: 1.011898\n",
      "Loss after iteration 0: 1.197031\n",
      "Loss after iteration 0: 0.913148\n",
      "Loss after iteration 0: 1.099081\n",
      "Loss after iteration 0: 1.165771\n",
      "Loss after iteration 0: 1.052535\n",
      "Loss after iteration 0: 1.066426\n",
      "Loss after iteration 0: 1.153863\n",
      "Loss after iteration 0: 1.017071\n",
      "Loss after iteration 0: 0.861176\n",
      "Loss after iteration 0: 0.979938\n",
      "Loss after iteration 0: 1.128327\n",
      "Loss after iteration 0: 1.034375\n",
      "Loss after iteration 0: 0.980759\n",
      "Loss after iteration 0: 0.996083\n",
      "Loss after iteration 0: 0.990313\n",
      "Loss after iteration 0: 1.067512\n",
      "Loss after iteration 0: 0.697051\n",
      "Loss after iteration 0: 0.805947\n",
      "Loss after iteration 0: 1.096154\n",
      "Loss after iteration 0: 1.083941\n",
      "Loss after iteration 0: 0.917828\n",
      "Loss after iteration 0: 0.913904\n",
      "Loss after iteration 0: 0.599195\n",
      "Loss after iteration 0: 0.824989\n",
      "Loss after iteration 0: 1.153462\n",
      "Loss after iteration 0: 0.982997\n",
      "Loss after iteration 0: 1.119156\n",
      "Loss after iteration 0: 1.083231\n",
      "Loss after iteration 0: 1.001198\n",
      "Loss after iteration 0: 1.043715\n",
      "Loss after iteration 0: 0.932276\n",
      "Loss after iteration 0: 0.870501\n",
      "Loss after iteration 0: 1.067721\n",
      "Loss after iteration 0: 1.022090\n",
      "Loss after iteration 0: 1.052400\n",
      "Loss after iteration 0: 1.190416\n",
      "Loss after iteration 0: 1.135024\n",
      "Loss after iteration 0: 1.091300\n",
      "Loss after iteration 0: 0.939927\n",
      "Loss after iteration 0: 0.889137\n",
      "Loss after iteration 0: 0.838256\n",
      "Loss after iteration 0: 0.996409\n",
      "Loss after iteration 0: 1.050552\n",
      "Loss after iteration 0: 1.097926\n",
      "Loss after iteration 0: 0.653534\n",
      "Loss after iteration 0: 1.204869\n",
      "Loss after iteration 0: 1.026130\n",
      "Loss after iteration 0: 0.856049\n",
      "Loss after iteration 0: 1.056603\n",
      "Loss after iteration 0: 1.012057\n",
      "Loss after iteration 0: 1.000237\n",
      "Loss after iteration 0: 1.109892\n",
      "Loss after iteration 0: 0.733815\n",
      "Loss after iteration 0: 0.710592\n",
      "Loss after iteration 0: 0.962990\n",
      "Loss after iteration 0: 0.785369\n",
      "Loss after iteration 0: 0.945441\n",
      "Loss after iteration 0: 0.798917\n",
      "Loss after iteration 0: 0.887505\n",
      "Loss after iteration 0: 0.951761\n",
      "Loss after iteration 0: 0.659434\n",
      "Loss after iteration 0: 0.968167\n",
      "Loss after iteration 0: 0.628443\n",
      "Loss after iteration 0: 1.025905\n",
      "Loss after iteration 0: 0.928775\n",
      "Loss after iteration 0: 0.791173\n",
      "Loss after iteration 0: 0.867433\n",
      "Loss after iteration 0: 0.773558\n",
      "Loss after iteration 0: 0.954683\n",
      "Loss after iteration 0: 1.072931\n",
      "Loss after iteration 0: 0.920232\n",
      "Loss after iteration 0: 1.082840\n",
      "Loss after iteration 0: 0.583541\n",
      "Loss after iteration 0: 0.947429\n",
      "Loss after iteration 0: 0.767447\n",
      "Loss after iteration 0: 0.886943\n",
      "Loss after iteration 0: 0.960815\n",
      "Loss after iteration 0: 0.651424\n",
      "Loss after iteration 0: 0.888739\n",
      "Loss after iteration 0: 0.900897\n",
      "Loss after iteration 0: 0.887103\n",
      "Loss after iteration 0: 0.793766\n",
      "Loss after iteration 0: 1.023548\n",
      "Loss after iteration 0: 0.991403\n",
      "Loss after iteration 0: 0.982882\n",
      "Loss after iteration 0: 0.685133\n",
      "Loss after iteration 0: 0.996526\n",
      "Loss after iteration 0: 0.875200\n",
      "Loss after iteration 0: 0.854898\n",
      "Loss after iteration 0: 1.036526\n",
      "Loss after iteration 0: 0.847988\n",
      "Loss after iteration 0: 1.055332\n",
      "Loss after iteration 0: 1.073249\n",
      "Loss after iteration 0: 0.898238\n",
      "Loss after iteration 0: 0.709843\n",
      "Loss after iteration 0: 1.066137\n",
      "Loss after iteration 0: 1.063577\n",
      "Loss after iteration 0: 1.036728\n",
      "Loss after iteration 0: 1.018323\n",
      "Loss after iteration 0: 1.152084\n",
      "Loss after iteration 0: 1.125929\n",
      "Loss after iteration 0: 0.810033\n",
      "Loss after iteration 0: 0.522047\n",
      "Loss after iteration 0: 0.797958\n",
      "Loss after iteration 0: 0.988481\n",
      "Loss after iteration 0: 0.889578\n",
      "Loss after iteration 0: 1.075126\n",
      "Loss after iteration 0: 0.811606\n",
      "Loss after iteration 0: 0.881646\n",
      "Loss after iteration 0: 0.904125\n",
      "Loss after iteration 0: 1.258259\n",
      "Loss after iteration 0: 1.024713\n",
      "Loss after iteration 0: 0.975029\n",
      "Loss after iteration 0: 0.908965\n",
      "Loss after iteration 0: 0.700401\n",
      "Loss after iteration 0: 0.990629\n",
      "Loss after iteration 0: 0.895804\n",
      "Loss after iteration 0: 0.620131\n",
      "Loss after iteration 0: 1.000386\n",
      "Loss after iteration 0: 0.399928\n",
      "Loss after iteration 0: 0.862214\n",
      "Loss after iteration 0: 0.618787\n",
      "Loss after iteration 0: 0.680680\n",
      "Loss after iteration 0: 0.684881\n",
      "Loss after iteration 0: 1.156417\n",
      "Loss after iteration 0: 1.097824\n",
      "Loss after iteration 0: 0.878172\n",
      "Loss after iteration 0: 0.763057\n",
      "Loss after iteration 0: 1.195520\n",
      "Loss after iteration 0: 0.867432\n",
      "Loss after iteration 0: 1.111853\n",
      "Loss after iteration 0: 0.591575\n",
      "Loss after iteration 0: 0.915002\n",
      "Loss after iteration 0: 0.865012\n",
      "Loss after iteration 0: 0.832509\n",
      "Loss after iteration 0: 0.915724\n",
      "Loss after iteration 0: 1.123954\n",
      "Loss after iteration 0: 1.013744\n",
      "Loss after iteration 0: 0.809033\n",
      "Loss after iteration 0: 0.865012\n",
      "Loss after iteration 0: 0.894287\n",
      "Loss after iteration 0: 1.087077\n",
      "Loss after iteration 0: 0.848407\n",
      "Loss after iteration 0: 0.617727\n",
      "Loss after iteration 0: 0.813112\n",
      "Loss after iteration 0: 0.741276\n",
      "Loss after iteration 0: 0.823952\n",
      "Loss after iteration 0: 1.025704\n",
      "Loss after iteration 0: 0.826708\n",
      "Loss after iteration 0: 0.575489\n",
      "Loss after iteration 0: 0.457560\n",
      "Loss after iteration 0: 1.001433\n",
      "Loss after iteration 0: 0.976972\n",
      "Loss after iteration 0: 0.723917\n",
      "Loss after iteration 0: 0.904109\n",
      "Loss after iteration 0: 1.012312\n",
      "Loss after iteration 0: 0.785554\n",
      "Loss after iteration 0: 1.093066\n",
      "Loss after iteration 0: 0.895609\n",
      "Loss after iteration 0: 0.492849\n",
      "Loss after iteration 0: 0.932684\n",
      "Loss after iteration 0: 0.995805\n",
      "Loss after iteration 0: 0.920054\n",
      "Loss after iteration 0: 1.057836\n",
      "Loss after iteration 0: 0.800077\n",
      "Loss after iteration 0: 0.959476\n",
      "Loss after iteration 0: 0.700564\n",
      "Loss after iteration 0: 0.839411\n",
      "Loss after iteration 0: 1.149813\n",
      "Loss after iteration 0: 0.900775\n",
      "Loss after iteration 0: 0.624104\n",
      "Loss after iteration 0: 0.828403\n",
      "Loss after iteration 0: 0.706029\n",
      "Loss after iteration 0: 0.938316\n",
      "Loss after iteration 0: 0.691467\n",
      "Loss after iteration 0: 0.424178\n",
      "Loss after iteration 0: 0.905825\n",
      "Loss after iteration 0: 0.759440\n",
      "Loss after iteration 0: 0.901854\n",
      "Loss after iteration 0: 0.935734\n",
      "Loss after iteration 0: 0.768647\n",
      "Loss after iteration 0: 0.784687\n",
      "Loss after iteration 0: 0.876784\n",
      "Loss after iteration 0: 1.918276\n",
      "Loss after iteration 0: 0.687797\n",
      "Loss after iteration 0: 1.033972\n",
      "Loss after iteration 0: 0.942044\n",
      "Loss after iteration 0: 1.015312\n",
      "Loss after iteration 0: 1.085089\n",
      "Loss after iteration 0: 1.002522\n",
      "Loss after iteration 0: 1.319628\n",
      "Loss after iteration 0: 0.792611\n",
      "Loss after iteration 0: 1.215874\n",
      "Loss after iteration 0: 0.980716\n",
      "Loss after iteration 0: 0.991225\n",
      "Loss after iteration 0: 0.995578\n",
      "Loss after iteration 0: 0.952912\n",
      "Loss after iteration 0: 1.062280\n",
      "Loss after iteration 0: 0.966993\n",
      "Loss after iteration 0: 0.880116\n",
      "Loss after iteration 0: 0.917940\n",
      "Loss after iteration 0: 0.919499\n",
      "Loss after iteration 0: 0.758827\n",
      "Loss after iteration 0: 1.038148\n",
      "Loss after iteration 0: 0.863310\n",
      "Loss after iteration 0: 0.859597\n",
      "Loss after iteration 0: 0.674818\n",
      "Loss after iteration 0: 0.744890\n",
      "Loss after iteration 0: 0.791080\n",
      "Loss after iteration 0: 1.006047\n",
      "Loss after iteration 0: 1.118236\n",
      "Loss after iteration 0: 0.875355\n",
      "Loss after iteration 0: 0.556602\n",
      "Loss after iteration 0: 1.169899\n",
      "Loss after iteration 0: 0.885464\n",
      "Loss after iteration 0: 1.003146\n",
      "Loss after iteration 0: 0.688837\n",
      "Loss after iteration 0: 0.912977\n",
      "Loss after iteration 0: 0.651118\n",
      "Loss after iteration 0: 0.857297\n",
      "Loss after iteration 0: 1.144867\n",
      "Loss after iteration 0: 0.931130\n",
      "Loss after iteration 0: 1.123267\n",
      "Loss after iteration 0: 1.009402\n",
      "Loss after iteration 0: 0.881639\n",
      "Loss after iteration 0: 1.410079\n",
      "Loss after iteration 0: 0.930511\n",
      "Loss after iteration 0: 0.866765\n",
      "Loss after iteration 0: 0.994515\n",
      "Loss after iteration 0: 0.790462\n",
      "Loss after iteration 0: 0.820928\n",
      "Loss after iteration 0: 0.711271\n",
      "Loss after iteration 0: 1.229175\n",
      "Loss after iteration 0: 0.766493\n",
      "Loss after iteration 0: 0.883610\n",
      "Loss after iteration 0: 0.828561\n",
      "Loss after iteration 0: 1.074018\n",
      "Loss after iteration 0: 0.834822\n",
      "Loss after iteration 0: 1.096247\n",
      "Loss after iteration 0: 1.170189\n",
      "Loss after iteration 0: 0.743260\n",
      "Loss after iteration 0: 0.904993\n",
      "Loss after iteration 0: 0.674016\n",
      "Loss after iteration 0: 0.675027\n",
      "Loss after iteration 0: 1.059082\n",
      "Loss after iteration 0: 1.124003\n",
      "Loss after iteration 0: 0.620438\n",
      "Loss after iteration 0: 0.889267\n",
      "Loss after iteration 0: 0.805880\n",
      "Loss after iteration 0: 0.838028\n",
      "Loss after iteration 0: 0.815097\n",
      "Loss after iteration 0: 0.603082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.616322\n",
      "Loss after iteration 0: 0.545369\n",
      "Loss after iteration 0: 0.771107\n",
      "Loss after iteration 0: 1.219675\n",
      "Loss after iteration 0: 0.848185\n",
      "Loss after iteration 0: 0.677566\n",
      "Loss after iteration 0: 0.698757\n",
      "Loss after iteration 0: 1.089731\n",
      "Loss after iteration 0: 0.942916\n",
      "Loss after iteration 0: 0.742213\n",
      "Loss after iteration 0: 1.002305\n",
      "Loss after iteration 0: 0.679022\n",
      "Loss after iteration 0: 0.681522\n",
      "Loss after iteration 0: 1.113946\n",
      "Loss after iteration 0: 0.817440\n",
      "Loss after iteration 0: 0.916198\n",
      "Loss after iteration 0: 0.862802\n",
      "Loss after iteration 0: 0.863845\n",
      "Loss after iteration 0: 1.230181\n",
      "Loss after iteration 0: 0.971658\n",
      "Loss after iteration 0: 1.051138\n",
      "Loss after iteration 0: 1.205459\n",
      "Loss after iteration 0: 0.928145\n",
      "Loss after iteration 0: 0.864323\n",
      "Loss after iteration 0: 1.065213\n",
      "Loss after iteration 0: 0.859945\n",
      "Loss after iteration 0: 1.207415\n",
      "Loss after iteration 0: 1.256552\n",
      "Loss after iteration 0: 0.680264\n",
      "Loss after iteration 0: 0.885899\n",
      "Loss after iteration 0: 0.778112\n",
      "Loss after iteration 0: 0.839538\n",
      "Loss after iteration 0: 0.899309\n",
      "Loss after iteration 0: 1.063760\n",
      "Loss after iteration 0: 0.518206\n",
      "Loss after iteration 0: 1.252906\n",
      "Loss after iteration 0: 0.724790\n",
      "Loss after iteration 0: 1.129971\n",
      "Loss after iteration 0: 0.862657\n",
      "Loss after iteration 0: 0.759101\n",
      "Loss after iteration 0: 0.917830\n",
      "Loss after iteration 0: 1.071563\n",
      "Loss after iteration 0: 0.833052\n",
      "Loss after iteration 0: 0.900715\n",
      "Loss after iteration 0: 0.837310\n",
      "Loss after iteration 0: 1.080728\n",
      "Loss after iteration 0: 0.753131\n",
      "Loss after iteration 0: 0.685653\n",
      "Loss after iteration 0: 0.901698\n",
      "Loss after iteration 0: 0.731187\n",
      "Loss after iteration 0: 0.832441\n",
      "Loss after iteration 0: 0.739230\n",
      "Loss after iteration 0: 1.037505\n",
      "Loss after iteration 0: 0.928353\n",
      "Loss after iteration 0: 1.030163\n",
      "Loss after iteration 0: 0.939064\n",
      "Loss after iteration 0: 1.068369\n",
      "Loss after iteration 0: 0.840406\n",
      "Loss after iteration 0: 1.238497\n",
      "Loss after iteration 0: 0.719279\n",
      "Loss after iteration 0: 1.176663\n",
      "Loss after iteration 0: 0.875027\n",
      "Loss after iteration 0: 1.035850\n",
      "Loss after iteration 0: 0.735507\n",
      "Loss after iteration 0: 0.825444\n",
      "Loss after iteration 0: 1.025065\n",
      "Loss after iteration 0: 0.966743\n",
      "Loss after iteration 0: 0.817957\n",
      "Loss after iteration 0: 1.020188\n",
      "Loss after iteration 0: 0.790900\n",
      "Loss after iteration 0: 0.821297\n",
      "Loss after iteration 0: 0.994584\n",
      "Loss after iteration 0: 0.986459\n",
      "Loss after iteration 0: 0.797712\n",
      "Loss after iteration 0: 0.801846\n",
      "Loss after iteration 0: 0.731005\n",
      "Loss after iteration 0: 0.858785\n",
      "Loss after iteration 0: 0.995844\n",
      "Loss after iteration 0: 0.976371\n",
      "Loss after iteration 0: 0.748219\n",
      "Loss after iteration 0: 0.532446\n",
      "Loss after iteration 0: 0.524098\n",
      "Loss after iteration 0: 0.926563\n",
      "Loss after iteration 0: 1.061410\n",
      "Loss after iteration 0: 0.714377\n",
      "Loss after iteration 0: 1.090845\n",
      "Loss after iteration 0: 0.757355\n",
      "Loss after iteration 0: 0.677418\n",
      "Loss after iteration 0: 0.797513\n",
      "Loss after iteration 0: 0.795551\n",
      "Loss after iteration 0: 0.942573\n",
      "Loss after iteration 0: 0.868131\n",
      "Loss after iteration 0: 0.838894\n",
      "Loss after iteration 0: 1.017860\n",
      "Loss after iteration 0: 1.200071\n",
      "Loss after iteration 0: 0.929089\n",
      "Loss after iteration 0: 0.773153\n",
      "Loss after iteration 0: 0.782388\n",
      "Loss after iteration 0: 0.792028\n",
      "Loss after iteration 0: 1.194423\n",
      "Loss after iteration 0: 1.169428\n",
      "Loss after iteration 0: 0.816900\n",
      "Loss after iteration 0: 1.194722\n",
      "Loss after iteration 0: 0.890369\n",
      "Loss after iteration 0: 0.945031\n",
      "Loss after iteration 0: 1.157754\n",
      "Loss after iteration 0: 1.056079\n",
      "Loss after iteration 0: 0.793866\n",
      "Loss after iteration 0: 0.770259\n",
      "Loss after iteration 0: 1.393511\n",
      "Loss after iteration 0: 0.990970\n",
      "Loss after iteration 0: 0.790891\n",
      "Loss after iteration 0: 0.689117\n",
      "Loss after iteration 0: 0.828177\n",
      "Loss after iteration 0: 0.850061\n",
      "Loss after iteration 0: 0.707544\n",
      "Loss after iteration 0: 0.849163\n",
      "Loss after iteration 0: 0.864723\n",
      "Loss after iteration 0: 0.859359\n",
      "Loss after iteration 0: 0.846343\n",
      "Loss after iteration 0: 0.969151\n",
      "Loss after iteration 0: 1.268981\n",
      "Loss after iteration 0: 0.747249\n",
      "Loss after iteration 0: 0.728694\n",
      "Loss after iteration 0: 0.961506\n",
      "Loss after iteration 0: 0.650588\n",
      "Loss after iteration 0: 0.840877\n",
      "Loss after iteration 0: 0.889620\n",
      "Loss after iteration 0: 0.572337\n",
      "Loss after iteration 0: 1.153258\n",
      "Loss after iteration 0: 0.907694\n",
      "Loss after iteration 0: 0.799372\n",
      "Loss after iteration 0: 0.802636\n",
      "Loss after iteration 0: 0.908631\n",
      "Loss after iteration 0: 0.640066\n",
      "Loss after iteration 0: 0.699953\n",
      "Loss after iteration 0: 0.835405\n",
      "Loss after iteration 0: 4.679933\n",
      "Loss after iteration 0: 8.607377\n",
      "Loss after iteration 0: 5.236064\n",
      "Loss after iteration 0: 2.763656\n",
      "Loss after iteration 0: 5.265319\n",
      "Loss after iteration 0: 1.906379\n",
      "Loss after iteration 0: 2.195721\n",
      "Loss after iteration 0: 1.981311\n",
      "Loss after iteration 0: 1.820717\n",
      "Loss after iteration 0: 2.149545\n",
      "Loss after iteration 0: 1.105336\n",
      "Loss after iteration 0: 0.801339\n",
      "Loss after iteration 0: 1.264803\n",
      "Loss after iteration 0: 1.024727\n",
      "Loss after iteration 0: 1.457251\n",
      "Loss after iteration 0: 1.364822\n",
      "Loss after iteration 0: 1.346514\n",
      "Loss after iteration 0: 0.745949\n",
      "Loss after iteration 0: 1.327589\n",
      "Loss after iteration 0: 1.015847\n",
      "Loss after iteration 0: 1.157429\n",
      "Loss after iteration 0: 1.138843\n",
      "Loss after iteration 0: 1.124287\n",
      "Loss after iteration 0: 1.005897\n",
      "Loss after iteration 0: 0.825459\n",
      "Loss after iteration 0: 1.340558\n",
      "Loss after iteration 0: 1.193068\n",
      "Loss after iteration 0: 1.199235\n",
      "Loss after iteration 0: 1.189384\n",
      "Loss after iteration 0: 1.098894\n",
      "Loss after iteration 0: 1.033445\n",
      "Loss after iteration 0: 0.974594\n",
      "Loss after iteration 0: 0.950543\n",
      "Loss after iteration 0: 1.116101\n",
      "Loss after iteration 0: 0.988830\n",
      "Loss after iteration 0: 1.170256\n",
      "Loss after iteration 0: 1.158200\n",
      "Loss after iteration 0: 1.091094\n",
      "Loss after iteration 0: 1.085009\n",
      "Loss after iteration 0: 1.097518\n",
      "Loss after iteration 0: 1.043116\n",
      "Loss after iteration 0: 1.067594\n",
      "Loss after iteration 0: 1.101704\n",
      "Loss after iteration 0: 1.088112\n",
      "Loss after iteration 0: 1.028953\n",
      "Loss after iteration 0: 1.051830\n",
      "Loss after iteration 0: 1.074588\n",
      "Loss after iteration 0: 1.096878\n",
      "Loss after iteration 0: 1.070447\n",
      "Loss after iteration 0: 1.068498\n",
      "Loss after iteration 0: 1.050176\n",
      "Loss after iteration 0: 0.986655\n",
      "Loss after iteration 0: 1.009236\n",
      "Loss after iteration 0: 1.106447\n",
      "Loss after iteration 0: 1.138632\n",
      "Loss after iteration 0: 1.115489\n",
      "Loss after iteration 0: 1.100323\n",
      "Loss after iteration 0: 1.046209\n",
      "Loss after iteration 0: 1.094667\n",
      "Loss after iteration 0: 1.062827\n",
      "Loss after iteration 0: 1.109073\n",
      "Loss after iteration 0: 1.044135\n",
      "Loss after iteration 0: 1.064689\n",
      "Loss after iteration 0: 1.046905\n",
      "Loss after iteration 0: 1.081592\n",
      "Loss after iteration 0: 1.090508\n",
      "Loss after iteration 0: 1.089883\n",
      "Loss after iteration 0: 1.115540\n",
      "Loss after iteration 0: 1.106520\n",
      "Loss after iteration 0: 1.068630\n",
      "Loss after iteration 0: 1.089587\n",
      "Loss after iteration 0: 0.968093\n",
      "Loss after iteration 0: 1.080081\n",
      "Loss after iteration 0: 1.121921\n",
      "Loss after iteration 0: 1.099627\n",
      "Loss after iteration 0: 1.086803\n",
      "Loss after iteration 0: 1.089595\n",
      "Loss after iteration 0: 1.085527\n",
      "Loss after iteration 0: 0.938362\n",
      "Loss after iteration 0: 1.079810\n",
      "Loss after iteration 0: 1.130070\n",
      "Loss after iteration 0: 1.119624\n",
      "Loss after iteration 0: 1.075893\n",
      "Loss after iteration 0: 1.029019\n",
      "Loss after iteration 0: 1.090429\n",
      "Loss after iteration 0: 0.859361\n",
      "Loss after iteration 0: 0.988500\n",
      "Loss after iteration 0: 1.156965\n",
      "Loss after iteration 0: 1.065454\n",
      "Loss after iteration 0: 1.125270\n",
      "Loss after iteration 0: 1.083506\n",
      "Loss after iteration 0: 1.122440\n",
      "Loss after iteration 0: 1.064658\n",
      "Loss after iteration 0: 1.087301\n",
      "Loss after iteration 0: 0.970510\n",
      "Loss after iteration 0: 1.006240\n",
      "Loss after iteration 0: 1.138054\n",
      "Loss after iteration 0: 1.105223\n",
      "Loss after iteration 0: 1.078957\n",
      "Loss after iteration 0: 1.041771\n",
      "Loss after iteration 0: 1.099536\n",
      "Loss after iteration 0: 1.117574\n",
      "Loss after iteration 0: 0.991449\n",
      "Loss after iteration 0: 1.117612\n",
      "Loss after iteration 0: 1.022034\n",
      "Loss after iteration 0: 1.047817\n",
      "Loss after iteration 0: 1.052177\n",
      "Loss after iteration 0: 1.105667\n",
      "Loss after iteration 0: 1.135182\n",
      "Loss after iteration 0: 1.113457\n",
      "Loss after iteration 0: 1.087673\n",
      "Loss after iteration 0: 0.951360\n",
      "Loss after iteration 0: 1.122612\n",
      "Loss after iteration 0: 1.106128\n",
      "Loss after iteration 0: 1.099474\n",
      "Loss after iteration 0: 1.097334\n",
      "Loss after iteration 0: 1.073287\n",
      "Loss after iteration 0: 1.051065\n",
      "Loss after iteration 0: 1.119153\n",
      "Loss after iteration 0: 1.070111\n",
      "Loss after iteration 0: 1.088196\n",
      "Loss after iteration 0: 1.092722\n",
      "Loss after iteration 0: 1.044216\n",
      "Loss after iteration 0: 1.117747\n",
      "Loss after iteration 0: 1.083814\n",
      "Loss after iteration 0: 1.027184\n",
      "Loss after iteration 0: 1.111481\n",
      "Loss after iteration 0: 1.033378\n",
      "Loss after iteration 0: 1.130488\n",
      "Loss after iteration 0: 1.096242\n",
      "Loss after iteration 0: 1.106579\n",
      "Loss after iteration 0: 1.085415\n",
      "Loss after iteration 0: 1.071823\n",
      "Loss after iteration 0: 1.045747\n",
      "Loss after iteration 0: 1.092852\n",
      "Loss after iteration 0: 1.090038\n",
      "Loss after iteration 0: 1.096155\n",
      "Loss after iteration 0: 1.067278\n",
      "Loss after iteration 0: 1.104688\n",
      "Loss after iteration 0: 1.046901\n",
      "Loss after iteration 0: 1.099953\n",
      "Loss after iteration 0: 1.102898\n",
      "Loss after iteration 0: 1.048553\n",
      "Loss after iteration 0: 1.102374\n",
      "Loss after iteration 0: 1.072711\n",
      "Loss after iteration 0: 1.069158\n",
      "Loss after iteration 0: 0.959435\n",
      "Loss after iteration 0: 1.086866\n",
      "Loss after iteration 0: 0.934013\n",
      "Loss after iteration 0: 1.153341\n",
      "Loss after iteration 0: 1.027310\n",
      "Loss after iteration 0: 1.081051\n",
      "Loss after iteration 0: 0.906281\n",
      "Loss after iteration 0: 1.061081\n",
      "Loss after iteration 0: 1.082583\n",
      "Loss after iteration 0: 1.003143\n",
      "Loss after iteration 0: 1.051888\n",
      "Loss after iteration 0: 0.919401\n",
      "Loss after iteration 0: 1.161155\n",
      "Loss after iteration 0: 1.033269\n",
      "Loss after iteration 0: 0.943937\n",
      "Loss after iteration 0: 0.987848\n",
      "Loss after iteration 0: 0.917009\n",
      "Loss after iteration 0: 1.106649\n",
      "Loss after iteration 0: 1.114040\n",
      "Loss after iteration 0: 1.146752\n",
      "Loss after iteration 0: 1.108017\n",
      "Loss after iteration 0: 1.010749\n",
      "Loss after iteration 0: 0.995445\n",
      "Loss after iteration 0: 1.063350\n",
      "Loss after iteration 0: 1.084230\n",
      "Loss after iteration 0: 1.171902\n",
      "Loss after iteration 0: 1.030490\n",
      "Loss after iteration 0: 1.072614\n",
      "Loss after iteration 0: 0.969995\n",
      "Loss after iteration 0: 0.975939\n",
      "Loss after iteration 0: 1.170040\n",
      "Loss after iteration 0: 1.085601\n",
      "Loss after iteration 0: 1.029803\n",
      "Loss after iteration 0: 0.837882\n",
      "Loss after iteration 0: 1.184468\n",
      "Loss after iteration 0: 1.121578\n",
      "Loss after iteration 0: 1.060953\n",
      "Loss after iteration 0: 1.124210\n",
      "Loss after iteration 0: 1.051694\n",
      "Loss after iteration 0: 1.086741\n",
      "Loss after iteration 0: 1.068400\n",
      "Loss after iteration 0: 1.044953\n",
      "Loss after iteration 0: 1.065697\n",
      "Loss after iteration 0: 1.087482\n",
      "Loss after iteration 0: 0.800293\n",
      "Loss after iteration 0: 1.147544\n",
      "Loss after iteration 0: 1.137737\n",
      "Loss after iteration 0: 1.122628\n",
      "Loss after iteration 0: 1.029297\n",
      "Loss after iteration 0: 1.113604\n",
      "Loss after iteration 0: 1.053589\n",
      "Loss after iteration 0: 1.111714\n",
      "Loss after iteration 0: 1.113156\n",
      "Loss after iteration 0: 1.036655\n",
      "Loss after iteration 0: 1.091907\n",
      "Loss after iteration 0: 1.046857\n",
      "Loss after iteration 0: 1.094193\n",
      "Loss after iteration 0: 1.090948\n",
      "Loss after iteration 0: 1.076628\n",
      "Loss after iteration 0: 1.063678\n",
      "Loss after iteration 0: 1.094968\n",
      "Loss after iteration 0: 1.088784\n",
      "Loss after iteration 0: 1.089286\n",
      "Loss after iteration 0: 1.086215\n",
      "Loss after iteration 0: 1.045122\n",
      "Loss after iteration 0: 1.035465\n",
      "Loss after iteration 0: 1.080922\n",
      "Loss after iteration 0: 1.130598\n",
      "Loss after iteration 0: 0.999200\n",
      "Loss after iteration 0: 1.099530\n",
      "Loss after iteration 0: 1.050933\n",
      "Loss after iteration 0: 1.087433\n",
      "Loss after iteration 0: 1.038204\n",
      "Loss after iteration 0: 1.083620\n",
      "Loss after iteration 0: 0.779701\n",
      "Loss after iteration 0: 1.198398\n",
      "Loss after iteration 0: 1.081415\n",
      "Loss after iteration 0: 1.095970\n",
      "Loss after iteration 0: 1.090344\n",
      "Loss after iteration 0: 1.087436\n",
      "Loss after iteration 0: 1.124651\n",
      "Loss after iteration 0: 1.085871\n",
      "Loss after iteration 0: 1.114604\n",
      "Loss after iteration 0: 1.103714\n",
      "Loss after iteration 0: 0.973647\n",
      "Loss after iteration 0: 1.060495\n",
      "Loss after iteration 0: 1.138141\n",
      "Loss after iteration 0: 1.066218\n",
      "Loss after iteration 0: 0.982764\n",
      "Loss after iteration 0: 1.028601\n",
      "Loss after iteration 0: 0.898619\n",
      "Loss after iteration 0: 1.157579\n",
      "Loss after iteration 0: 1.168644\n",
      "Loss after iteration 0: 1.102678\n",
      "Loss after iteration 0: 0.940623\n",
      "Loss after iteration 0: 1.071524\n",
      "Loss after iteration 0: 1.084854\n",
      "Loss after iteration 0: 1.126658\n",
      "Loss after iteration 0: 1.005215\n",
      "Loss after iteration 0: 1.042283\n",
      "Loss after iteration 0: 1.035463\n",
      "Loss after iteration 0: 0.885211\n",
      "Loss after iteration 0: 1.143475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.082103\n",
      "Loss after iteration 0: 1.135871\n",
      "Loss after iteration 0: 1.109215\n",
      "Loss after iteration 0: 1.071735\n",
      "Loss after iteration 0: 1.097793\n",
      "Loss after iteration 0: 1.096256\n",
      "Loss after iteration 0: 1.047351\n",
      "Loss after iteration 0: 1.084182\n",
      "Loss after iteration 0: 1.083138\n",
      "Loss after iteration 0: 1.098943\n",
      "Loss after iteration 0: 1.049603\n",
      "Loss after iteration 0: 1.027031\n",
      "Loss after iteration 0: 1.076276\n",
      "Loss after iteration 0: 0.987720\n",
      "Loss after iteration 0: 0.970198\n",
      "Loss after iteration 0: 1.098057\n",
      "Loss after iteration 0: 1.079951\n",
      "Loss after iteration 0: 1.079271\n",
      "Loss after iteration 0: 0.999338\n",
      "Loss after iteration 0: 1.171738\n",
      "Loss after iteration 0: 1.016843\n",
      "Loss after iteration 0: 1.090101\n",
      "Loss after iteration 0: 1.014141\n",
      "Loss after iteration 0: 1.140196\n",
      "Loss after iteration 0: 0.822747\n",
      "Loss after iteration 0: 1.154706\n",
      "Loss after iteration 0: 1.075597\n",
      "Loss after iteration 0: 1.061740\n",
      "Loss after iteration 0: 0.923673\n",
      "Loss after iteration 0: 1.081504\n",
      "Loss after iteration 0: 1.069368\n",
      "Loss after iteration 0: 1.138792\n",
      "Loss after iteration 0: 1.070654\n",
      "Loss after iteration 0: 1.100953\n",
      "Loss after iteration 0: 0.935978\n",
      "Loss after iteration 0: 1.107185\n",
      "Loss after iteration 0: 1.125006\n",
      "Loss after iteration 0: 0.933139\n",
      "Loss after iteration 0: 1.086510\n",
      "Loss after iteration 0: 1.106509\n",
      "Loss after iteration 0: 1.088127\n",
      "Loss after iteration 0: 1.114237\n",
      "Loss after iteration 0: 1.041120\n",
      "Loss after iteration 0: 1.091873\n",
      "Loss after iteration 0: 1.091603\n",
      "Loss after iteration 0: 1.095767\n",
      "Loss after iteration 0: 1.045898\n",
      "Loss after iteration 0: 1.096082\n",
      "Loss after iteration 0: 1.055829\n",
      "Loss after iteration 0: 0.990822\n",
      "Loss after iteration 0: 1.127242\n",
      "Loss after iteration 0: 1.092383\n",
      "Loss after iteration 0: 1.111469\n",
      "Loss after iteration 0: 0.980244\n",
      "Loss after iteration 0: 1.117629\n",
      "Loss after iteration 0: 1.060567\n",
      "Loss after iteration 0: 1.093237\n",
      "Loss after iteration 0: 1.057083\n",
      "Loss after iteration 0: 1.033055\n",
      "Loss after iteration 0: 1.087183\n",
      "Loss after iteration 0: 1.060305\n",
      "Loss after iteration 0: 1.080285\n",
      "Loss after iteration 0: 1.124451\n",
      "Loss after iteration 0: 1.088011\n",
      "Loss after iteration 0: 1.094922\n",
      "Loss after iteration 0: 1.032223\n",
      "Loss after iteration 0: 0.983680\n",
      "Loss after iteration 0: 1.155988\n",
      "Loss after iteration 0: 1.084699\n",
      "Loss after iteration 0: 1.083160\n",
      "Loss after iteration 0: 1.088197\n",
      "Loss after iteration 0: 0.811055\n",
      "Loss after iteration 0: 1.000053\n",
      "Loss after iteration 0: 1.087623\n",
      "Loss after iteration 0: 1.039198\n",
      "Loss after iteration 0: 1.087985\n",
      "Loss after iteration 0: 1.086234\n",
      "Loss after iteration 0: 1.169214\n",
      "Loss after iteration 0: 1.046245\n",
      "Loss after iteration 0: 1.093037\n",
      "Loss after iteration 0: 1.036881\n",
      "Loss after iteration 0: 1.091449\n",
      "Loss after iteration 0: 1.137581\n",
      "Loss after iteration 0: 1.071842\n",
      "Loss after iteration 0: 1.074999\n",
      "Loss after iteration 0: 0.938639\n",
      "Loss after iteration 0: 1.170772\n",
      "Loss after iteration 0: 1.067560\n",
      "Loss after iteration 0: 1.106221\n",
      "Loss after iteration 0: 1.091517\n",
      "Loss after iteration 0: 1.054511\n",
      "Loss after iteration 0: 1.092288\n",
      "Loss after iteration 0: 1.041321\n",
      "Loss after iteration 0: 1.019163\n",
      "Loss after iteration 0: 1.122523\n",
      "Loss after iteration 0: 1.058938\n",
      "Loss after iteration 0: 1.031882\n",
      "Loss after iteration 0: 1.019420\n",
      "Loss after iteration 0: 1.079142\n",
      "Loss after iteration 0: 1.066109\n",
      "Loss after iteration 0: 1.079576\n",
      "Loss after iteration 0: 1.070634\n",
      "Loss after iteration 0: 1.127624\n",
      "Loss after iteration 0: 1.117827\n",
      "Loss after iteration 0: 1.100747\n",
      "Loss after iteration 0: 0.986884\n",
      "Loss after iteration 0: 1.052531\n",
      "Loss after iteration 0: 0.817508\n",
      "Loss after iteration 0: 1.084943\n",
      "Loss after iteration 0: 1.192209\n",
      "Loss after iteration 0: 1.113002\n",
      "Loss after iteration 0: 1.010305\n",
      "Loss after iteration 0: 1.068896\n",
      "Loss after iteration 0: 1.133398\n",
      "Loss after iteration 0: 1.083999\n",
      "Loss after iteration 0: 1.062400\n",
      "Loss after iteration 0: 1.053853\n",
      "Loss after iteration 0: 1.083779\n",
      "Loss after iteration 0: 1.100704\n",
      "Loss after iteration 0: 1.094434\n",
      "Loss after iteration 0: 1.078658\n",
      "Loss after iteration 0: 0.900719\n",
      "Loss after iteration 0: 0.999904\n",
      "Loss after iteration 0: 1.071882\n",
      "Loss after iteration 0: 1.143802\n",
      "Loss after iteration 0: 1.086756\n",
      "Loss after iteration 0: 1.111264\n",
      "Loss after iteration 0: 0.938259\n",
      "Loss after iteration 0: 1.092248\n",
      "Loss after iteration 0: 0.922777\n",
      "Loss after iteration 0: 1.089916\n",
      "Loss after iteration 0: 1.138115\n",
      "Loss after iteration 0: 1.079065\n",
      "Loss after iteration 0: 0.979041\n",
      "Loss after iteration 0: 1.122190\n",
      "Loss after iteration 0: 1.019388\n",
      "Loss after iteration 0: 1.131137\n",
      "Loss after iteration 0: 1.083373\n",
      "Loss after iteration 0: 1.092000\n",
      "Loss after iteration 0: 1.102087\n",
      "Loss after iteration 0: 1.044184\n",
      "Loss after iteration 0: 1.067906\n",
      "Loss after iteration 0: 1.114699\n",
      "Loss after iteration 0: 0.883151\n",
      "Loss after iteration 0: 1.138054\n",
      "Loss after iteration 0: 1.079462\n",
      "Loss after iteration 0: 1.119452\n",
      "Loss after iteration 0: 1.080685\n",
      "Loss after iteration 0: 1.063970\n",
      "Loss after iteration 0: 1.104778\n",
      "Loss after iteration 0: 1.017140\n",
      "Loss after iteration 0: 1.109867\n",
      "Loss after iteration 0: 1.093481\n",
      "Loss after iteration 0: 1.094889\n",
      "Loss after iteration 0: 1.023470\n",
      "Loss after iteration 0: 1.082425\n",
      "Loss after iteration 0: 1.070344\n",
      "Loss after iteration 0: 1.113621\n",
      "Loss after iteration 0: 1.091797\n",
      "Loss after iteration 0: 1.068708\n",
      "Loss after iteration 0: 0.961850\n",
      "Loss after iteration 0: 1.106883\n",
      "Loss after iteration 0: 1.079731\n",
      "Loss after iteration 0: 1.040708\n",
      "Loss after iteration 0: 1.040795\n",
      "Loss after iteration 0: 1.079606\n",
      "Loss after iteration 0: 1.132271\n",
      "Loss after iteration 0: 1.086514\n",
      "Loss after iteration 0: 1.099611\n"
     ]
    }
   ],
   "source": [
    "#various number of neurons in hidden layer\n",
    "#learning rate change to 0.001\n",
    "# batch_size changed to 8\n",
    "neurons = [8,16,32]\n",
    "#keep model with each hyper parameter setting, i.e. 8, 16, 32\n",
    "model = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1 = {}\n",
    "for n in neurons:\n",
    "    \n",
    "    nnet = SimpleNN()\n",
    "    nnet.add(activation=\"tanh\",input_dim=8, hidden_dim=n, output_dim=3, print_loss=True)\n",
    "    nnet.train(uci_features_train, uci_labels_train, epoches=2000, reg_value=0.01, epsilon=0.001, batch_size=8)\n",
    "    \n",
    "    #evaluate the model on validation set\n",
    "    prediction = test_model.infer(uci_features_valid)\n",
    "    confusion = confusion_matrix(uci_labels_valid, prediction)\n",
    "    confusion = np.array(confusion)\n",
    "    p = Precision(confusion)\n",
    "    r = Recall(confusion)\n",
    "    f1_score = F1_score(confusion)\n",
    "    \n",
    "    #save model to model, precision, recall and f1 on dict, where n is the number of neurons used in hidden layer\n",
    "    model[n] = {nnet}\n",
    "    precision[n] = {p}\n",
    "    recall[n] = {r}\n",
    "    f1[n] = {f1_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31f4e0",
   "metadata": {},
   "source": [
    "__model with various hyperparameter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "87cf4715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {<mllab.Hackathon.SimpleNN at 0x7ffa2a722040>},\n",
       " 16: {<mllab.Hackathon.SimpleNN at 0x7ffa2567a9d0>},\n",
       " 32: {<mllab.Hackathon.SimpleNN at 0x7ffa28d49460>}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038c0a1",
   "metadata": {},
   "source": [
    "__precision with various hyperparameter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a8cd968e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {0.5714285714285714}, 16: {0.5714285714285714}, 32: {0.5714285714285714}}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4daa4a",
   "metadata": {},
   "source": [
    "__recall with various hyperparameter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "456b7737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {0.5714285714285714}, 16: {0.5714285714285714}, 32: {0.5714285714285714}}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303202e5",
   "metadata": {},
   "source": [
    "__f1-score with various hyperparameter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9209b91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {0.5714285714285714}, 16: {0.5714285714285714}, 32: {0.5714285714285714}}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-spell",
   "metadata": {},
   "source": [
    "# Step VII: Experiment (3) Neural Net Stability on Shuffled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-death",
   "metadata": {},
   "source": [
    "- over multiple runs $r \\geq 5, r\\in\\mathbb{N}$ randomly shuffle your training data\n",
    "- split it into a train-test, e.g. 90% of the data is for training, 10% for testing\n",
    "- what are the mean and standard deviation of your models over multiple runs?\n",
    "- plot a boxplot with matplotlib/seaborn of the stability of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-census",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = np.minimum(np.random.normal(0.8, 0.1, (100,)), 1)\n",
    "sns.boxplot(data=example_results)\n",
    "plt.title(\"Stability of My Model over 100 runs on test set\")\n",
    "plt.ylabel(\"Accuracy on test set\")\n",
    "plt.xlabel(\"My Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-composer",
   "metadata": {},
   "source": [
    "# Step VIII: Bonus: implement Momentum SGD / ADAM / .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-quilt",
   "metadata": {},
   "source": [
    "- this task is optional if you have time at the end\n",
    "- inspect your stochastic gradient descent implementation\n",
    "- have a look at online examples such as [wiseodd.github.com](https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/) for implementations of variants on stochastic gradient such as with Nesterov Momentum or ADAM\n",
    "- change your implementation of SGD to one or multiple of these variants and try a simple run of your neural net and compare it with previous results\n",
    "- sketch a first design of an optimizer-class which is fed with parameters of your model and performs the update step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-stopping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
